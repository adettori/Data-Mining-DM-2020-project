{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1: Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for displaying plots below the cell\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('customer_supermarket.csv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to contain data about the shopping habits of the customers of a grocery store chain.  \n",
    "Each row represents an object purchased:  \n",
    "- BasketID: identifies a batch of items bought during the same shopping session  \n",
    "- BasketDate: date in which the shopping session took place  \n",
    "- Sale: represents the value of the item, we need to figure out if it refers to a single item or the item*quantity\n",
    "- CustomerID: identifies a unique customer\n",
    "- CustomerCountry: represents the country in which the purchase took place\n",
    "- ProdID: identifies a unique product for sale\n",
    "- ProdDescr: describes the product\n",
    "- Qta: number of items bought with id ProdID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only ProdDescr and CustomerID contain null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics regarding the CustomerID are meaningless since the assignment of an ID is usually done progressively and without having any additional information on the customer.  \n",
    "We need to fix the data type situation in order to get a better understanding of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type conversion  \n",
    "Let's start by checking out the data type that pandas assigns to the attributes, in order to get an idea of the potential problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.convert_dtypes().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomerID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomerID got converted to a reasonable data type while the others became a generic \"string\".  \n",
    "However there is no point in having CustomerID as an int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CustomerID\"] = df[\"CustomerID\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasketDate\n",
    "Let's convert the BasketDate type from String to datetime, just in case we need to perform some analysis that requires ordinal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BasketDate = pd.to_datetime(df.BasketDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Sale\" attribute is considered a generic object while it should be recognised as a float.  \n",
    "Let's see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sale.map(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.Sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Sale uses a comma instead of a point to separate the decimal part, so it is considered a \"str\" instead of a \"float64\".  \n",
    "Let's replace the commas in \"Sale\" with dots in order to have them be recognised as float64 by pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sale = df.Sale.apply(lambda x: x.replace(',','.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sale = df.Sale.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sale is now correctly identified as a float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration data frame\n",
    "Used for exploration purposes but not necessarily useful for clustering.  \n",
    "Initialised with some additional features that could prove useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auxiliary df to be used throughout the data understanding phase\n",
    "df_expl = df.copy()\n",
    "\n",
    "df_expl[\"QtaPositive\"] = 0\n",
    "df_expl.loc[df_expl[\"Qta\"] > 0, \"QtaPositive\"] = 1 #Indicates whether the records Qta is positive\n",
    "\n",
    "df_expl[\"SalePositive\"] = 0\n",
    "df_expl.loc[df_expl[\"Sale\"] > 0, \"SalePositive\"] = 1 #Indicates whether the records Sale is positive\n",
    "\n",
    "df_expl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasketID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the different types of BasketID\n",
    "Let's check why BasketID is not considered an int64 like CustomerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonNumSeries = pd.to_numeric(df.BasketID, errors='coerce').isnull()\n",
    "# Print the records with BasketIDs containing a non-numeric value\n",
    "df[nonNumSeries].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[nonNumSeries, \"BasketID\"].str.slice(0,1).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a good chunk of the BasketID values start with a \"C\" and some with \"A\" instead of being just numbers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_c_df = df.loc[df[\"BasketID\"].str.get(0) == \"C\"]\n",
    "len_basket_c = len(basket_c_df)\n",
    "print(f\"Records starting with 'C' (Size: {len_basket_c}):\\n\")\n",
    "basket_c_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_a_df = df.loc[df[\"BasketID\"].str.get(0) == \"A\"]\n",
    "len_basket_a = len(basket_a_df)\n",
    "print(f\"Records starting with 'A' (Size: {len_basket_a}):\\n\")\n",
    "basket_a_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a strong correlation between the \"C\" and a negative quantity, this could indicate a customer that asked for a refund.  \n",
    "\n",
    "There is also some interesting correlation between the \"A\" start and a ProdDescr containing \"Adjust bad debt\", maybe the \"A\" stands for adjust and since the CustomerID in both cases is NaN this could be an operation that concerns only the management of the shop and not something that concerns the customers (which is our primary objective).  \n",
    "These records, however, are too few to be meaningful, they skew too much the characteristics of the sale data (outliers) and they don't concern the activities of the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to add a \"BasketID type A\" and \"BasketID type C\" binary attribute (0/1) and see if there are correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise all the cells to 0\n",
    "df_expl[\"BasketIDTypeA\"] = 0\n",
    "df_expl[\"BasketIDTypeC\"] = 0\n",
    "\n",
    "#Set the cells appropriately depending on the BasketID type\n",
    "df_expl.loc[df[\"BasketID\"].str.get(0) == \"A\", \"BasketIDTypeA\"] = 1\n",
    "df_expl.loc[df[\"BasketID\"].str.get(0) == \"C\", \"BasketIDTypeC\"] = 1\n",
    "\n",
    "df_expl[\"NewBasketID\"] = df_expl[\"BasketID\"]\n",
    "\n",
    "#Remove the initial letter from BasketID where necessary\n",
    "df_expl.loc[df_expl[\"BasketID\"].str.get(0) == \"A\", \"NewBasketID\"] = df_expl.loc[(df_expl[\"BasketID\"].str.get(0) == \"A\"), \"BasketID\"].str.slice(start=1)\n",
    "df_expl.loc[df_expl[\"BasketID\"].str.get(0) == \"C\", \"NewBasketID\"] = df_expl.loc[(df_expl[\"BasketID\"].str.get(0) == \"C\"), \"BasketID\"].str.slice(start=1)\n",
    "\n",
    "df_expl.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BasketID of type C has a strong negative correlation with the sign of Qta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"BasketID\"].str.get(0) == \"C\", \"ProdDescr\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could this mean for the C type? Probably indicates discounts/refunds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"NewBasketID\"] = df_expl[\"NewBasketID\"].astype(\"int64\")\n",
    "df_expl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there are no more anomalies inside BasketID since it can be now converted to int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"NewBasketID\"] = df_expl[\"NewBasketID\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we now have less unique BasketIDs in our records, after removing the letter that identifies the type from the BasketID attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The original number of unique BasketIDs is: {df_expl[\"BasketID\"].unique().size}')\n",
    "print(f'The current number of unique BasketIDs is: {df_expl[\"NewBasketID\"].unique().size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number is the same, therefore each BasketID of type A or C didn't merge with pre-existing shopping sessions.  \n",
    "We can therefore replace the old naming scheme with the new one which doesn't contain letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"BasketID\"] = df_expl[\"NewBasketID\"]\n",
    "df_expl = df_expl.drop(\"NewBasketID\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the merger of type C records into standard records\n",
    "It could prove useful to take into account the BasketDate and see if it would make sense to merge the type C records with the ones referencing the same item in a previous order from the same customer.  \n",
    "Idea: check if all the negatives have a corresponding positive order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"ProdSaleQta\"] = df_expl[\"Qta\"]*df_expl[\"Sale\"]\n",
    "df_expl[\"AbsProdSaleQta\"] = df_expl[\"ProdSaleQta\"].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only about 9000 entries to check, computationally feasible\n",
    "df_typec = df_expl.loc[df_expl[\"BasketIDTypeC\"] == 1].copy()\n",
    "std_df = df_expl.loc[(df_expl[\"BasketIDTypeA\"] == 0) & (df_expl[\"BasketIDTypeC\"] == 0)].copy()\n",
    "\n",
    "#New dataframe that we are going to save and reuse later for further analysis\n",
    "type_c_id_list = df_typec[\"BasketID\"].sort_values().unique()\n",
    "refunds_dict = {}\n",
    "\n",
    "i = 0\n",
    "\n",
    "for index, series in df_typec.iterrows():\n",
    "    tmp_df1 = std_df.loc[(std_df[\"BasketDate\"] <= series[\"BasketDate\"]) & \n",
    "                         (std_df[\"Sale\"] == series[\"Sale\"]) & \n",
    "                         (std_df[\"CustomerID\"] == series[\"CustomerID\"]) & \n",
    "                         (std_df[\"ProdID\"] == series[\"ProdID\"])]\n",
    "    \n",
    "    tmp_df1 = tmp_df1.sort_values([\"BasketDate\"], ascending=False)\n",
    "    \n",
    "    #Stronger requirement, if present otherwise merge with a record that satisfies the requirements of tmp_df1\n",
    "    tmp_df2 = tmp_df1.loc[(tmp_df1[\"AbsProdSaleQta\"] == series[\"AbsProdSaleQta\"])]\n",
    "    #Whenever in doubt about the API properties...\n",
    "    tmp_df2 = tmp_df2.sort_values([\"BasketDate\"], ascending=False)\n",
    "    \n",
    "    order_found = None\n",
    "\n",
    "    if(tmp_df2.empty == False):\n",
    "        #Given the additional constraint the first entry is sufficient\n",
    "        order_found = tmp_df2.iloc[0]\n",
    "        \n",
    "        #print(f\"{order_found['BasketID']} {order_found['ProdID']} {order_found['Qta']} \\\n",
    "        #      {std_df.loc[(std_df['BasketID'] == order_found['BasketID']) & (std_df['ProdID'] == order_found['ProdID'])]['Qta']}\")\n",
    "        \n",
    "        std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"AbsProdSaleQta\"] = 0\n",
    "        std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"Qta\"] = 0\n",
    "        \n",
    "        #Save result\n",
    "        print(order_found[\"BasketID\"])\n",
    "        refunds_dict[(series[\"BasketID\"], series[\"ProdID\"])] = pd.array([order_found['BasketID']])\n",
    "        \n",
    "    elif(tmp_df1.empty == False):\n",
    "        order_list = []\n",
    "        amount_to_refund = -1*series[\"Qta\"]\n",
    "        tmp_i = 0\n",
    "        \n",
    "        while(amount_to_refund > 0 and len(tmp_df1.index) > tmp_i):\n",
    "            order_found = tmp_df1.iloc[tmp_i]\n",
    "            tmp_i = tmp_i + 1\n",
    "            \n",
    "            if(order_found[\"Qta\"] == 0):\n",
    "                continue\n",
    "            \n",
    "            order_list.append(order_found[\"BasketID\"])\n",
    "            amount_to_refund = amount_to_refund - order_found[\"Qta\"]\n",
    "            \n",
    "            #Need to avoid two different refunds referencing the same order without any items left to be refunded\n",
    "            if(amount_to_refund >= 0):\n",
    "                tmp_df1.loc[tmp_df1[\"BasketID\"] == order_found[\"BasketID\"], \"Qta\"] = 0\n",
    "                std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"Qta\"] = 0\n",
    "            else:\n",
    "                tmp_df1.loc[tmp_df1[\"BasketID\"] == order_found[\"BasketID\"], \"Qta\"] = -1*amount_to_refund\n",
    "                std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"Qta\"] = -1*amount_to_refund\n",
    "        \n",
    "        #Save result\n",
    "        print(order_list)\n",
    "        if(len(order_list) > 0):\n",
    "            refunds_dict[(series[\"BasketID\"], series[\"ProdID\"])] = pd.array(order_list)\n",
    "    else:\n",
    "        print(f\"No match found\")\n",
    "        \n",
    "    if(i > 100):\n",
    "        break\n",
    "    \n",
    "    print(i)\n",
    "    i = i+1\n",
    "\n",
    "#Transform a python dictionary with a key of dimension=2 and one value for each key into a DataFrame with 3 columns\n",
    "#Avoids the problem of inserting lists with different lengths in the dataframe\n",
    "refund_df = pd.DataFrame(pd.Series(refunds_dict).reset_index()).set_axis(['RefundID', \"ProdID\",'OrigBasketID'],1,inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refund_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasketDate\n",
    "Let's see how the entries are distributed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = math.ceil(math.log(len(df[\"BasketID\"]), 2) + 1) #Sturge's rule\n",
    "df.groupby(by=[\"BasketID\"]).nth(0)[\"BasketDate\"].hist(bins=k, figsize=(10,5))\n",
    "plt.title('Distribution of BasketIDs with respect to years')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of transactions increases month by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = sorted(df[\"BasketDate\"].dt.weekday.unique())\n",
    "days_dict = dict(zip([0,1,2,3,4,5,6], [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]))\n",
    "\n",
    "basketday = df.groupby(by=[\"BasketID\"]).nth(0)[\"BasketDate\"].dt.weekday #Series containing the day of the week of each BasketID\n",
    "basketday = basketday.sort_values().transform(lambda x: days_dict[x])\n",
    "basketday.hist(bins=np.arange(0, len(days)+1, 1), figsize=(10,5))\n",
    "plt.title('Distribution of BasketIDs with respect to weekday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saturday and sunday seem to be the least active days for the shops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = sorted(df[\"BasketDate\"].dt.month.unique())\n",
    "months_dict = dict(zip([1,2,3,4,5,6, 7, 8, 9, 10, 11, 12], \n",
    "                       [\"January\", \"February\", \"March\", \"April\", \n",
    "                        \"May\", \"June\", \"July\", \"August\", \n",
    "                        \"September\", \"October\", \"November\", \"December\"]))\n",
    "\n",
    "basketmonth = df.groupby(by=[\"BasketID\"]).nth(0)[\"BasketDate\"].dt.month #Series containing the day of the week of each BasketID\n",
    "basketmonth = basketmonth.sort_values().transform(lambda x: months_dict[x])\n",
    "basketmonth.hist(bins=np.arange(0, len(months)+1, 1), figsize=(15,5))\n",
    "plt.title('Distribution of BasketIDs with respect to month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sales peak happens in November and the least amount of sales occurs in December, presumably the customers plan their Christmas shopping in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distributions of Sale and Qta taking into account the BasketDate\n",
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "\n",
    "plt.scatter(df['BasketDate'], \n",
    "            df['Sale'], color='g', marker='*', label='Data')\n",
    "plt.xlabel('BasketDate')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Sale')\n",
    "\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "\n",
    "plt.scatter(df['BasketDate'], \n",
    "            df['Qta'], color='g', marker='*', label='Data')\n",
    "plt.xlabel('BasketDate')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Qta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sale\n",
    "We need to figure out if the Sale value refers to the cost of a single item or cost of item * Qta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"ProdID\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Sale doesn't change if the Qta is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be a correlation in general between Sale and Qta, we can therefore Sale is the cost of the single item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the Sale distribution\n",
    "fig = plt.figure(figsize=(15, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "k = math.ceil(math.log(len(df[\"Sale\"]), 2) + 1) #Sturge's rule\n",
    "df[\"Sale\"].hist(bins=np.arange(0, k, 1))\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "df.boxplot(column=[\"Sale\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the vast majority of Sale values are small.  \n",
    "We need however to check for 0 values since they don't make sense in the contest of Sale and therefore should be considered as missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"Sale\"] == 0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost a quarter of the Sale values are 0, this needs to be fixed in the Data Preparation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomerID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding null CustomerIDs\n",
    "Let's see why the number of non-null CustomerID entries is so low and if there are any interesting properties to be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"CustomerIDNull\"] = 0\n",
    "df_expl.loc[df_expl[\"CustomerID\"].isna(), \"CustomerIDNull\"] = 1\n",
    "\n",
    "df_expl.corr()[\"CustomerIDNull\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No interesting correlation.  \n",
    "Let's check if we can retrieve some missing CustomerIDs by using the records referencing the same BasketID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.groupby(by=\"BasketID\").filter(lambda x: x[\"CustomerID\"].isna().any() & x[\"CustomerID\"].notna().any()).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no intersection between records with the same BasketID but different CustomerIDNull value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basketid_country = df.groupby(by=[\"CustomerCountry\"])[\"BasketID\"].nunique()\n",
    "basketid_country.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the operations take place in the United Kingdom.  \n",
    "It could be interesting to however take into account the revenue by country and see which is more profitable relative to the number of orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryList = df[\"CustomerCountry\"].sort_values().unique()\n",
    "country_df = pd.DataFrame(data=countryList, columns=[\"Country\"])\n",
    "\n",
    "df[\"ProductSaleQta\"] = df[\"Sale\"]*df[\"Qta\"]\n",
    "\n",
    "for country in countryList:\n",
    "    country_df.loc[country_df[\"Country\"] == country, \"TotalSale\"] = df.loc[df[\"CustomerCountry\"] == country, \"ProductSaleQta\"].sum()\n",
    "    country_df.loc[country_df[\"Country\"] == country, \"TotalOrders\"] = basketid_country[country]\n",
    "\n",
    "df = df.drop(\"ProductSaleQta\", axis=1)\n",
    "\n",
    "country_df[\"AvgSalePerOrder\"] = country_df[\"TotalSale\"]/country_df[\"TotalOrders\"]\n",
    "\n",
    "country_df.sort_values(by=\"AvgSalePerOrder\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProdID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ProdID interpretation  \n",
    "Let's find out why this wasn't converted to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"ProdID\"].str.isnumeric(), (\"ProdID\", \"ProdDescr\")].value_counts() #Records with ProdIDs containing only numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"ProdID\"].str.isalpha(), (\"ProdID\", \"ProdDescr\")].value_counts() #Records with ProdIDs containing only letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Records with ProdID terminating with a letter\n",
    "term_letter_prodid = df.loc[(df[\"ProdID\"].str.slice(start=-1).str.isalpha()) & (df[\"ProdID\"].str.slice(0, -1).str.isnumeric())]\n",
    "term_letter_prodid[[\"ProdID\", \"ProdDescr\"]].sort_values(by=\"ProdID\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The letters seem to represent different variations of the same item.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the diversity and lack of structure of the ProdIDs, as can be seen in the different types listed above, there doesn't seem to be anything worth exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding ProdIDs in type C BasketID records\n",
    "We would also like to get an idea of the percentage of ProdID referenced in type C records that are also referenced in standard type records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodid_c_set = df_expl.loc[df_expl[\"BasketIDTypeC\"] == 1, \"ProdID\"].sort_values().unique()\n",
    "prodid_std_set = df_expl.loc[(df_expl[\"BasketIDTypeC\"] == 0) & (df_expl[\"BasketIDTypeA\"] == 0), \"ProdID\"].sort_values().unique()\n",
    "\n",
    "intersection_size = len(list(set(prodid_c_set) & set(prodid_std_set)))\n",
    "\n",
    "intersection_size/len(prodid_c_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore say that the vast majority of items referenced in C type records are also referenced in standard records, further strengthening the discount/refund hypothesis for C records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = math.ceil(math.log(len(df[\"Qta\"]), 2) + 1) #Sturge's rule\n",
    "df[\"Qta\"].hist(bins=np.arange(0, k))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"Qta\"] == 0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no records with Qta equal to 0, so we can assume that there are no records with missing values in the feature Qta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding negative Qta and type C BasketID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.corr()[\"QtaPositive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the BasketID section there is a strong correlation between the sign of Qta and a BasketID of type C.  \n",
    "Let's see if there is some interesting distribution in the remaining negative quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_not_c = df_expl.loc[(df_expl[\"Qta\"] < 0) & (df_expl[\"BasketIDTypeC\"] == 0)]\n",
    "neg_not_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the trend of Sale equal 0 continues throughout the subset of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_not_c[\"Sale\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does.  \n",
    "Let's check if all CustomerIDs in the subset are Null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_not_c.describe()[\"CustomerIDNull\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are all Null, as can be deduced by the min value.  \n",
    "It might be a good idea to remove this data in the Data preparation phase, since we don't care about records that do not describe a customer's behaviour.  \n",
    "This way we will also have a correlation of 1 between the BasketID class C and negative quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding negative Qta and standard type BasketID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = neg_not_c[\"ProdDescr\"].str.lower().str.cat(sep=\" \").split(\" \")\n",
    "word_series = pd.Series(data=word_list).str.strip('.!? \\n\\t')\n",
    "word_series.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to indicate items that became unsellable for various reasons and got removed from the stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_not_c.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that they have no Sale value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleanup (Pointless since the customer features consider only CustomerID non-null entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove type A BasketID entries since, as noted above, they don't specify a CustomerID and are concerned with the grocery store chain debt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.loc[df[\"BasketID\"].str.get(0) == \"A\"].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove records with negative Qta that do not reference a type C BasketID since they do not reference any CustomerID.  \n",
    "Now all records with negative Qta are of type C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.loc[(df[\"BasketID\"].str.get(0) != \"C\") & (df[\"Qta\"] < 0)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't seem to have a way to retrieve the CustomerID in case it is missing let's delete all entries without CustomerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.loc[df[\"CustomerID\"].isna()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values will be replaced by the median of the Sale values obtained by grouping with respect to ProdID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sale'] = df.groupby(['ProdID'])['Sale'].transform(lambda x: \n",
    "                                                      x.replace(to_replace=0, method='ffill', value = x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the median instead of the mean to be less susceptible to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers\n",
    "Using the IQR method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_out = df.copy()\n",
    "\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the outliers in Sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "df.boxplot(\"Sale\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "df_wo_out = df_wo_out.loc[(df_wo_out[\"Sale\"] >= Q1[\"Sale\"] - 1.5*IQR[\"Sale\"]) & (df_wo_out[\"Sale\"] <= Q3[\"Sale\"] + 1.5*IQR[\"Sale\"])]\n",
    "df_wo_out.boxplot(\"Sale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the outliers in Qta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "df.boxplot(\"Qta\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "df_wo_out = df_wo_out.loc[(df_wo_out[\"Qta\"] >= Q1[\"Qta\"] - 1.5*IQR[\"Qta\"]) & (df_wo_out[\"Qta\"] <= Q3[\"Qta\"] + 1.5*IQR[\"Qta\"])]\n",
    "df_wo_out.boxplot(\"Qta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_out = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some new features to be used for customer analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(dest_df, source_df):\n",
    "    \n",
    "    #Additional useful data for features\n",
    "    source_df = source_df.copy()\n",
    "    source_df[\"DateYear\"] = source_df[\"BasketDate\"].dt.year\n",
    "    source_df[\"DateMonth\"] = source_df[\"BasketDate\"].dt.month\n",
    "    source_df[\"DateDay\"] = source_df[\"BasketDate\"].dt.day\n",
    "    source_df[\"DateWeekDay\"] = source_df[\"BasketDate\"].dt.weekday\n",
    "    \n",
    "    #Total number of items bought by customer\n",
    "    IFeature = source_df.groupby([\"CustomerID\"]).Qta.sum()\n",
    "    dest_df = dest_df.merge(IFeature, on=\"CustomerID\").rename(columns={\"Qta\":\"I\"})\n",
    "\n",
    "    #Total number of unique items bought by customer\n",
    "    IuFeature = source_df.groupby([\"CustomerID\"]).ProdID.nunique()\n",
    "    dest_df = dest_df.join(IuFeature, on=\"CustomerID\").rename(columns={\"ProdID\":\"Iu\"})\n",
    "\n",
    "    #Max number of item bought by customer across all shopping sessions\n",
    "    BasketIDQtaSum = source_df.groupby([\"CustomerID\", \"BasketID\"]).Qta.sum()\n",
    "    ImaxFeature = BasketIDQtaSum.groupby([\"CustomerID\"]).max()\n",
    "    dest_df = dest_df.join(ImaxFeature, on=\"CustomerID\").rename(columns={\"Qta\":\"Imax\"})\n",
    "\n",
    "    #Average total sale per month\n",
    "    tot_sale_cust = source_df.groupby([\"CustomerID\"]).apply(lambda x: (x[\"Sale\"]*x[\"Qta\"]).sum())\n",
    "    n_month_cust = source_df.groupby([\"CustomerID\"]).apply(lambda x: x.groupby([\"DateYear\"])[\"DateMonth\"].nunique().sum())\n",
    "    AvgFeature = tot_sale_cust/n_month_cust\n",
    "    AvgFeature.name = \"AvgExpendMonth\"\n",
    "    dest_df = dest_df.join(AvgFeature, on=\"CustomerID\")\n",
    "\n",
    "    #Average customer shopping sessions per month\n",
    "    tot_sessions_cust = source_df.groupby([\"CustomerID\"]).apply(lambda x: x[\"BasketID\"].nunique())\n",
    "    n_month_cust = source_df.groupby([\"CustomerID\"]).apply(lambda x: x.groupby([\"DateYear\"])[\"DateMonth\"].nunique().sum())\n",
    "    AvgFeature = tot_sessions_cust/n_month_cust\n",
    "    AvgFeature.name = \"AvgSessionsMonth\"\n",
    "    dest_df = dest_df.join(AvgFeature, on=\"CustomerID\")\n",
    "\n",
    "    #The Shannon entropy on the purchasing behaviour of the customer: days of the week of shopping\n",
    "    #Note: it takes into account also the type C orders\n",
    "    probSeriesGrouped = source_df.groupby([\"CustomerID\"])\\\n",
    "        .apply(lambda x: x.groupby([\"DateWeekDay\"])[\"BasketID\"].nunique()/x[\"BasketID\"].nunique())\n",
    "    logSeriesGrouped = np.log2(probSeriesGrouped)\n",
    "    ProdProbLogSeriesGrouped = -1 * probSeriesGrouped * logSeriesGrouped\n",
    "    EFeature = ProdProbLogSeriesGrouped.groupby([\"CustomerID\"]).sum()\n",
    "    EFeature.name = \"Eweekday\"\n",
    "    dest_df = dest_df.join(EFeature, on=\"CustomerID\")\n",
    "    \n",
    "    #The Shannon entropy on the purchasing behaviour of the customer: types of items bought\n",
    "    #Since we are assuming that the type C BasketID entries are refunds/returns we can ignore them for the purpouse of\n",
    "    # computing the entropy since they refer to the same \"item order\"\n",
    "    #TODO: need to handle the sum being negative\n",
    "    \n",
    "    #Idea: if the values are weird make the log null\n",
    "    source_df[\"Qta\"] = source_df[source_df[\"Qta\"] > 0]\n",
    "    probSeriesGrouped = source_df.groupby([\"CustomerID\"])\\\n",
    "            .apply(lambda x: x.groupby([\"ProdID\"])[\"Qta\"].sum()/x[\"Qta\"].sum())\n",
    "    logSeriesGrouped = np.log2(probSeriesGrouped)\n",
    "    ProdProbLogSeriesGrouped = -1 * probSeriesGrouped * logSeriesGrouped\n",
    "    EFeature = ProdProbLogSeriesGrouped.groupby([\"CustomerID\"]).sum()\n",
    "    EFeature = EFeature.fillna(0)\n",
    "    EFeature.name = \"Eproduct\"\n",
    "    dest_df = dest_df.join(EFeature, on=\"CustomerID\")\n",
    "\n",
    "    return dest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the new features to both the dataset with and without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_cust_id = df_w_out[\"CustomerID\"].sort_values().unique()[0:-1] #Remove NaN value, last value\n",
    "cust_df_w_out = pd.DataFrame(data=unq_cust_id, columns=[\"CustomerID\"]) #Dataframe containing customer features with outliers\n",
    "\n",
    "unq_cust_id = df_wo_out[\"CustomerID\"].sort_values().unique()[0:-1] #Remove NaN value, last value\n",
    "cust_df_wo_out = pd.DataFrame(data=unq_cust_id, columns=[\"CustomerID\"]) #Dataframe containing customer features without outliers\n",
    "\n",
    "cust_df_w_out[\"CustomerID\"] = cust_df_w_out[\"CustomerID\"].astype(\"object\")\n",
    "cust_df_wo_out[\"CustomerID\"] = cust_df_wo_out[\"CustomerID\"].astype(\"object\")\n",
    "\n",
    "cust_df_w_out = add_features(cust_df_w_out, df_w_out)\n",
    "cust_df_wo_out = add_features(cust_df_wo_out, df_wo_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_w_out = cust_df_w_out.corr()\n",
    "\n",
    "threshold = 0.6\n",
    "\n",
    "for col in corr_w_out.columns:\n",
    "    corr_w_out[col] = corr_w_out[col].transform(lambda x: x if x>threshold or x<-threshold else 0)\n",
    "\n",
    "corr_w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_wo_out = cust_df_wo_out.corr()\n",
    "\n",
    "threshold = 0.6\n",
    "\n",
    "for col in corr_wo_out.columns:\n",
    "    corr_wo_out[col] = corr_wo_out[col].transform(lambda x: x if x>threshold or x<-threshold else 0)\n",
    "\n",
    "corr_wo_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the higher the number of unique items bought by a customer the higher the variety of weekdays in which a shopping session takes place.  \n",
    "A similar but weaker relation is present between the total number of items bought and Eweekday, possibly skewed by the correlation between Iu and I.  \n",
    "It also seems that there is a significant correlation between the max number of items bought and the average expenditure per month of the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "diff_corr = (corr_w_out - corr_wo_out).abs()\n",
    "\n",
    "for col in diff_corr.columns:\n",
    "    diff_corr[col] = diff_corr[col].transform(lambda x: x if x>threshold else 0)\n",
    "\n",
    "diff_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(cust_df_w_out, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting correlation (0.737407) is the one between the features AvgExpendMonth and Imax, this could suggest to us that the customers with the biggest expenditures tend to buy more items per session (further analysis required). (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove customer related outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = cust_df.quantile(0.25)\n",
    "Q3 = cust_df.quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fig_height = cust_df.columns.size\n",
    "#fig = plt.figure(figsize=(20, 30)) \n",
    "#fig_dims = (fig_height, 2)\n",
    "#fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "#index = 0\n",
    "\n",
    "for feature in cust_df.columns:\n",
    "    #plt.subplot2grid(fig_dims, (index, 0))\n",
    "    #cust_df.boxplot(feature)\n",
    "\n",
    "    cust_df = cust_df.loc[(cust_df[feature] >= Q1[feature] - 1.5*IQR[feature]) & (cust_df[feature] <= Q3[feature] + 1.5*IQR[feature])]\n",
    "    \n",
    "    #plt.subplot2grid(fig_dims, (index, 1))\n",
    "    #cust_df.boxplot(feature)\n",
    "    #index = index + 1\n",
    "    \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(cust_df, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between Iu and Eweekday is even stronger after removing the outliers.  \n",
    "In order to reduce the dimensionality of the data we will drop Eweekday given the strong similarity to Iu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df = cust_df.drop(\"Eweekday\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization  \n",
    "Z-scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(cust_df.values)\n",
    "X = scaler.transform(cust_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Knee method to find the best k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_list = list()\n",
    "max_k = 40\n",
    "for k in range(2, max_k + 1): #Starting from k=2 to k=40\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, len(sse_list) + 2), sse_list)\n",
    "plt.ylabel('SSE', fontsize=22)\n",
    "plt.xlabel('K', fontsize=22)\n",
    "plt.tick_params(axis='both', which='major', labelsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that around K=10 ± 5 we get diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, n_init=10, max_iter=100)\n",
    "kmeans.fit(X)\n",
    "centers = scaler.inverse_transform(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(kmeans.labels_, \n",
    "                          bins=range(0, len(set(kmeans.labels_)) + 1))\n",
    "dict(zip(bins, hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(cust_df, c=kmeans.labels_, figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(0, len(centers)):\n",
    "    plt.plot(centers[i], marker='o', label='Cluster %s' % i)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "plt.xticks(range(0, len(cust_df.columns)), cust_df.columns, fontsize=18)\n",
    "plt.legend(fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of variable\n",
    "N = len(cust_df.columns)\n",
    "# What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "\n",
    "# Initialise the spider plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(polar=True)\n",
    "\n",
    "for i in range(0, len(centers)):\n",
    "    angles = [n / float(N) * 2 * math.pi for n in range(N)]\n",
    "    values = centers[i].tolist()\n",
    "    values += values[:1]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "# Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles[:-1], cust_df.columns, color='grey', size=8) \n",
    "# Plot data\n",
    "    ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    " # Fill area\n",
    "    ax.fill(angles, values, 'b', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X) #We need the normalised data\n",
    "print(pca.explained_variance_ratio_) #Variance explained by the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are retaining approximately 86% of the variance.  \n",
    "Let's apply the elbow method to the PCA transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_list = list()\n",
    "max_k = 40\n",
    "for k in range(2, max_k + 1): #Starting from k=2 to k=40\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(X_pca)\n",
    "    \n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, len(sse_list) + 2), sse_list)\n",
    "plt.ylabel('SSE', fontsize=22)\n",
    "plt.xlabel('K', fontsize=22)\n",
    "plt.tick_params(axis='both', which='major', labelsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value is still k=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_kmeans = KMeans(n_clusters=10, n_init=10, max_iter=100)\n",
    "pca_kmeans.fit(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=pca_kmeans.labels_, edgecolor='k', s=40)\n",
    "plt.title(\"PCA\")\n",
    "plt.xlabel(\"1st eigenvector\")\n",
    "plt.ylabel(\"2nd eigenvector\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters are well separated.  \n",
    "We now have a new categorical feature to help us analyse the pre-PCA dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
