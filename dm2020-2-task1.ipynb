{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Used for displaying plots below the cell\n",
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avenger/.local/lib/python3.9/site-packages/numpy/lib/arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df_geo = pd.read_csv('./geography.csv', sep=',', index_col=0)\n",
    "df_ram = pd.read_csv('./ram.csv', sep=',', index_col=0)\n",
    "\n",
    "# Original dataset split for VCS use\n",
    "df_sales_part1 = pd.read_csv('./sales_ram-part1.csv', sep=',', index_col=0) #The dataset doesn't have a column name. This causes the error.\n",
    "df_sales_part2 = pd.read_csv('./sales_ram-part2.csv', sep=',', index_col=0)\n",
    "df_sales = df_sales_part1.append(df_sales_part2)\n",
    "\n",
    "df_time = pd.read_csv('./time.csv', sep=',', index_col=0)\n",
    "df_vendor = pd.read_csv('./vendor.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ram_code</th>\n",
       "      <th>time_code</th>\n",
       "      <th>geo_code</th>\n",
       "      <th>vendor_code</th>\n",
       "      <th>sales_uds</th>\n",
       "      <th>sales_currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2602347</th>\n",
       "      <td>3719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130322</td>\n",
       "      <td>25</td>\n",
       "      <td>32</td>\n",
       "      <td>13.749032</td>\n",
       "      <td>10.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602348</th>\n",
       "      <td>3719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130323</td>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>13.828708</td>\n",
       "      <td>10.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602349</th>\n",
       "      <td>3719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130326</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>13.694297</td>\n",
       "      <td>10.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602350</th>\n",
       "      <td>3719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130327</td>\n",
       "      <td>25</td>\n",
       "      <td>32</td>\n",
       "      <td>13.690530</td>\n",
       "      <td>10.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602351</th>\n",
       "      <td>3719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130328</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>13.605216</td>\n",
       "      <td>10.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014673</th>\n",
       "      <td>7422</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>20170310</td>\n",
       "      <td>69</td>\n",
       "      <td>47</td>\n",
       "      <td>614.130000</td>\n",
       "      <td>614.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014674</th>\n",
       "      <td>7422</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>20170505</td>\n",
       "      <td>75</td>\n",
       "      <td>47</td>\n",
       "      <td>614.130000</td>\n",
       "      <td>614.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014675</th>\n",
       "      <td>7422</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>20170510</td>\n",
       "      <td>70</td>\n",
       "      <td>47</td>\n",
       "      <td>614.130000</td>\n",
       "      <td>614.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014676</th>\n",
       "      <td>7422</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>20170511</td>\n",
       "      <td>69</td>\n",
       "      <td>47</td>\n",
       "      <td>614.130000</td>\n",
       "      <td>614.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014677</th>\n",
       "      <td>7422</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>20170608</td>\n",
       "      <td>73</td>\n",
       "      <td>47</td>\n",
       "      <td>614.130000</td>\n",
       "      <td>614.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3412331 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  ram_code  time_code  geo_code  vendor_code   sales_uds  \\\n",
       "2602347  3719       1.0   20130322        25           32   13.749032   \n",
       "2602348  3719       1.0   20130323        18           32   13.828708   \n",
       "2602349  3719       1.0   20130326        28           32   13.694297   \n",
       "2602350  3719       1.0   20130327        25           32   13.690530   \n",
       "2602351  3719       1.0   20130328        27           32   13.605216   \n",
       "...       ...       ...        ...       ...          ...         ...   \n",
       "6014673  7422    3704.0   20170310        69           47  614.130000   \n",
       "6014674  7422    3704.0   20170505        75           47  614.130000   \n",
       "6014675  7422    3704.0   20170510        70           47  614.130000   \n",
       "6014676  7422    3704.0   20170511        69           47  614.130000   \n",
       "6014677  7422    3704.0   20170608        73           47  614.130000   \n",
       "\n",
       "         sales_currency  \n",
       "2602347           10.65  \n",
       "2602348           10.65  \n",
       "2602349           10.65  \n",
       "2602350           10.65  \n",
       "2602351           10.65  \n",
       "...                 ...  \n",
       "6014673          614.13  \n",
       "6014674          614.13  \n",
       "6014675          614.13  \n",
       "6014676          614.13  \n",
       "6014677          614.13  \n",
       "\n",
       "[3412331 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1: Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will go through all the datasets imported and try to mine some interesting information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Continents in the dataset: {df_geo['continent'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Countries in the dataset: {df_geo['country'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Regions in the dataset: {df_geo['region'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Currencies in the dataset: {df_geo['currency'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_geo[\"geo_code\"].duplicated().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo.groupby([\"continent\",\"country\", \"region\", \"currency\"]).size().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each combination of attributes in the dataset is unique, therefore there are no entries with different geo_code but same content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_geo[\"continent\"].isna().any())\n",
    "print(df_geo[\"country\"].isna().any())\n",
    "print(df_geo[\"region\"].isna().any())\n",
    "print(df_geo[\"currency\"].isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values in sight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ram brands: {df_ram[\"brand\"].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ram names: {df_ram[\"name\"].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ram memory type: {df_ram[\"memory_type\"].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ram.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are all within reasonable range, there are no obvious outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ram[\"brand\"].isna().any())\n",
    "print(df_ram[\"name\"].isna().any())\n",
    "print(df_ram[\"memory_type\"].isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ram.groupby([\"brand\", \"name\", \"memory_type\", \"clock\"]).size().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly there are multiple entries with identical rows (as in brand, name, type of memory and clock) but different ram_code identifiers.  \n",
    "This might be problematic in case we need to construct the vendor features using the ram_code attribute.  \n",
    "We will fix the problem in the data preparation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ram.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RAMs included in the dataset do not have a correlation between their clock and memory capacity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now merge all of the datasets in a single one in order to get a better picture of the sales and the features of the RAMs sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_merged = df_sales[[\"Id\", \"ram_code\"]].join(df_ram, on=\"ram_code\", rsuffix=\"_ram\")\n",
    "df_sales_merged[\"time_code\"] = df_sales[\"time_code\"]\n",
    "df_sales_merged = df_sales_merged.join(df_time.set_index(\"time_code\"), on=\"time_code\")\n",
    "df_sales_merged[\"geo_code\"] = df_sales[\"geo_code\"]\n",
    "df_sales_merged = df_sales_merged.join(df_geo.set_index(\"geo_code\"), on=\"geo_code\")\n",
    "df_sales_merged[\"vendor_code\"] = df_sales[\"vendor_code\"]\n",
    "df_sales_merged = df_sales_merged.join(df_vendor.set_index(\"vendor_code\"), on=\"vendor_code\", rsuffix=\"_vendor\")\n",
    "df_sales_merged = df_sales_merged.join(df_sales[[\"sales_uds\", \"sales_currency\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_merged.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales_uds meaning\n",
    "The objective of this section is to understand what sales_uds is.  \n",
    "The name seems to indicate that it is the value in US dollars of the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that sales_uds and sales_currency are strongly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_currency_series = (df_sales[\"sales_uds\"]-df_sales[\"sales_currency\"]).abs()\n",
    "diff_currency_series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A min value of 0 indicates that there are some rows in which the two values (sales_uds, sales_currency) are identical.  \n",
    "Let's find out which countries this property holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_country = df_sales.loc[diff_currency_series == 0].set_index(\"geo_code\").join(df_geo.set_index(\"geo_code\"))\n",
    "df_diff_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_country[\"country\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the only country in the dataset that has sales_usd always equal to sales_currency is the US this confirms our intuition.  \n",
    "This result is fundamental given that throughout our analysis we will refer to this value in order to have a similar metric across countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between ram characteristics and price  (TODO: we are missing the memory type, need to take into account)  \n",
    "In this section we look into the correlation between the features of a RAM module and its price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_merged.corrwith(df_sales_merged[\"sales_uds\"])[[\"memory\", \"clock\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there is no correlation between memory/clock characteristics and sales_uds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_merged.groupby(\"vendor_code\")[[\"memory\", \"clock\"]].corrwith(df_sales_merged[\"sales_uds\"]).plot()\n",
    "plt.ylabel(\"Corr\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when grouping by vendor_code the correlation between characteristics and price is very strong except for a few vendors.  \n",
    "In general the clock is not as strongly correlated, positively or negatively, to the price of the ram as it can be seen from the distribution of the correlations.  \n",
    "\n",
    "Let's try to introduce a temporary feature combining both clock and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sales = df_sales_merged.copy()\n",
    "df_test_sales[\"ddr_num\"] = df_test_sales[\"memory\"] * df_test_sales[\"clock\"]\n",
    "df_test_sales[\"MemoryByClock\"] = df_test_sales[\"memory\"] * df_test_sales[\"clock\"]\n",
    "\n",
    "df_test_sales.groupby(\"vendor_code\")[[\"MemoryByClock\"]].corrwith(df_test_sales[\"sales_uds\"]).plot()\n",
    "plt.ylabel(\"Corr\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, except for a few vendors, the majority of has a very strong correlation between the RAM features and its price.  \n",
    "This is what we expected from the start, now the question becomes why there are vendors that skew this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_result = df_test_sales.groupby(\"vendor_code\")[[\"MemoryByClock\"]].corrwith(df_test_sales[\"sales_uds\"])\n",
    "# We want to take a closer look to the extreme cases\n",
    "# 0.70 is the cut-off point chosen for a strong correlation\n",
    "corr_outliers = corr_result.loc[corr_result[\"MemoryByClock\"] < 0.70].sort_values(\"MemoryByClock\").rename(columns={\"MemoryByClock\":\"CorrRamUsd\"}) \n",
    "corr_outliers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers_subset = df_test_sales.loc[df_test_sales[\"vendor_code\"].isin(corr_outliers.index.values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the worst offenders, let's try to find out why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_subset_result = df_outliers_subset.groupby([\"vendor_code\", \"country\", \"year\"])[\"MemoryByClock\"].corr(df_outliers_subset[\"sales_uds\"])\n",
    "corr_subset_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the vendors sales by (year, country) we can see that there are groups in which the general trend is broken (ie vendor 62, year 2016, UK).  \n",
    "Let's take a week by week view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.70 is the cut-off point chosen for a strong correlation, as done previously\n",
    "corr_tmp = df_outliers_subset.groupby([\"vendor_code\", \"country\", \"region\", \"year\"])[\"MemoryByClock\"].corr(df_outliers_subset[\"sales_uds\"])\n",
    "corr_tmp_res = corr_tmp.loc[corr_tmp < 0.70]\n",
    "\n",
    "corr_tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers_subset.loc[(df_outliers_subset[\"vendor_code\"] == 72) & (df_outliers_subset[\"year\"] == 2018) & (df_outliers_subset[\"country\"] == \"United Kingdom\") & (df_outliers_subset[\"region\"] == \"yorkshire\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers_subset.loc[(df_outliers_subset[\"vendor_code\"] == 62) & (df_outliers_subset[\"year\"] == 2016)].quantile(0.99)[\"sales_uds\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there are a bunch of \"rip-off\" purchases going on that are skewing the correlation.  \n",
    "Removing the outliers should solve the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sales_df = df_sales_merged.drop([\"brand\", \"name\", \"memory_type\", \"continent\", \"country\", \"region\", \"currency\", \"name_vendor\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sn.heatmap(clean_sales_df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the correlation coefficients present are not relevant, some however give us some information about the meaning of the data:  \n",
    "- (ram_code, id): it implies that there is a 1-to-1 relationship between them, in other words for each id we have a single ram_code and viceversa.  \n",
    "    This makes one of the two attributes redundant which seems strange given that they represent very different things, this is probably connected to the sales dataset duplication previously found and will be analysed shortly.  \n",
    "- (sales_uds, sales_currency): this correlation is interesting, we have plenty of currencies both weaker and stronger than the us dollars, however we still obtained a value of 1.  \n",
    "    It signifies that there is a very special pattern here, usefult to groupby currency. (TODO) \n",
    "- (week, month): this correlation is a given since month is about week/4.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Id: Id and ram_code are virtually the same, they identify the type of ram sold by the vendor. For each Id there is only a type of ram sold.\n",
    "* ram_code: see above\n",
    "* time_code: identifies the date in which the ram was sold by the vendor\n",
    "* geo_code: identifies the location in which the ram was sold/sent\n",
    "* vendor_code: identifies the vendor that carried out the transaction\n",
    "* sales_uds: identifies the value of the sale in us dollars\n",
    "* sales_currency: identifies the value of the sale in the local currency of the country in which it was sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No suspicious values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicated time_code values: {df_time['time_code'].duplicated().any()}\")\n",
    "print(f\"Presence of NaN in the dataset: {df_time.isna().any().any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vendor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vendor.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vendor[\"name\"].unique())\n",
    "print(len(df_vendor[\"name\"].unique()))\n",
    "\n",
    "print(df_vendor[\"name\"].isna().any())\n",
    "print(\"\" in df_vendor[\"name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vendor.groupby([\"vendor_code\" ,\"name\"]).size().transform(lambda x: True if x>1 else False).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: run %matplotlib notebook to enable interactivity\n",
    "def plot_3d_scatter(data_points_list, labels_list=None):\n",
    "    \n",
    "    if(labels_list):\n",
    "        if(len(data_points_list) != len(labels_list)):\n",
    "            raise RuntimeError\n",
    "        \n",
    "    list_size = len(data_points_list)\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    \n",
    "    for i in range(list_size):\n",
    "        \n",
    "        ax = fig.add_subplot((list_size+1)//2, 2, i+1, projection='3d') #row, column, index\n",
    "        X = data_points_list[i].copy()\n",
    "        \n",
    "        if(labels_list):\n",
    "            \n",
    "            if(len(data_points_list[i]) != len(labels_list[i])):\n",
    "                raise RuntimeError\n",
    "\n",
    "            X[\"label\"] = labels_list[i]\n",
    "\n",
    "            for l in np.unique(labels_list[i]):\n",
    "                ax.scatter(X.loc[X.label == l, 0], X.loc[X.label == l, 1], X.loc[X.label == l, 2],\n",
    "                           cmap=plt.get_cmap(\"Pastel1\"),\n",
    "                           s=20, edgecolor='k')\n",
    "        else:\n",
    "            ax.scatter(X[0], X[1], X[2], s=20, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_merged.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be any missing value, such as NaN or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the datasets except Sales contain the description of a particular code (ie ram_code), we are therefore gonna assume that there is no point in looking for outliers there since they are just \"definitions\".  \n",
    "We will focus on the Sales dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_sales[\"sales_uds\"], df_sales[\"sales_currency\"], color='g', marker='*', label='Standard')\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df_sales.quantile(0.25)\n",
    "Q3 = df_sales.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "right_whisker = Q3 + 1.5*IQR\n",
    "left_whisker = Q1 - 1.5*IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sale USD 2.5-quantile: {df_sales['sales_uds'].quantile(0.025)}\")\n",
    "print(f\"Sale USD 97.5-quantile: {df_sales['sales_uds'].quantile(0.975)}\")\n",
    "print(f\"Sale Local 2.5-quantile: {df_sales['sales_currency'].quantile(0.025)}\")\n",
    "print(f\"Sale Local 97.5-quantile: {df_sales['sales_currency'].quantile(0.975)}\\n\")\n",
    "\n",
    "print(f\"Left whisker:\\n{left_whisker[['sales_uds', 'sales_currency']]}\\n\")\n",
    "print(f\"Right whisker:\\n{right_whisker[['sales_uds', 'sales_currency']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left whisket is negative. Weird. (TODO?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_out_iqr = df_sales.copy()\n",
    "\n",
    "df_wo_out_iqr = df_wo_out_iqr.loc[(df_wo_out_iqr[\"sales_uds\"] >= left_whisker[\"sales_uds\"]) & (df_wo_out_iqr[\"sales_uds\"] <= right_whisker[\"sales_uds\"])]\n",
    "df_wo_out_iqr = df_wo_out_iqr.loc[(df_wo_out_iqr[\"sales_currency\"] >= left_whisker[\"sales_currency\"]) & (df_wo_out_iqr[\"sales_currency\"] <= right_whisker[\"sales_currency\"])]\n",
    "\n",
    "df_size_diff = df_sales.shape[0]-df_wo_out_iqr.shape[0]\n",
    "print(f'The number of total outliers in the dataset is: {df_size_diff}')\n",
    "print(f'That is {df_size_diff*100/df_sales.shape[0]:3.2f}% of the total entries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 3)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "plt.title(\"Sampled dataset with outliers\")\n",
    "init_axes = \\\n",
    "    plt.scatter(df_sales[\"sales_uds\"], df_sales[\"sales_currency\"], \n",
    "                color=\"g\", marker='*', label='Standard').axes\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "plt.title(\"Sampled dataset without outliers\")\n",
    "plt.scatter(df_wo_out_iqr[\"sales_uds\"], df_wo_out_iqr[\"sales_currency\"], color=\"g\", marker='*', label='Standard')\n",
    "#Keep the same axis size for all the subplots\n",
    "plt.xlim(min(init_axes.viewLim.intervalx), max(init_axes.viewLim.intervalx))\n",
    "plt.ylim(min(init_axes.viewLim.intervaly), max(init_axes.viewLim.intervaly))\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 2))\n",
    "plt.title(\"Sampled dataset without outliers (Zoomed)\")\n",
    "plt.scatter(df_wo_out_iqr[\"sales_uds\"], df_wo_out_iqr[\"sales_currency\"], color=\"g\", marker='*', label='Standard')\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of outlier detection doesn't seem suitable to our purpouses since it classifies as outliers a significant amount of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_sample = df_sales[[\"sales_uds\", \"sales_currency\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(zscore_sample)\n",
    "norm_zscore_sample = scaler.transform(zscore_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zscore = pd.DataFrame(norm_zscore_sample, index=df_sales.index)\n",
    "df_zscore.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limBase = df_zscore.std()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "plt.title(\"Max and min values in sales_uds\")\n",
    "plt.xlabel(\"Number of time the std\")\n",
    "for r in np.arange(0.5, 4, 0.5):\n",
    "    df_range = df_zscore.loc[((df_zscore[0] > -r*limBase[0]) & (df_zscore[0] < r*limBase[0]))]\n",
    "    plt_max = df_sales.loc[df_range.index, \"sales_uds\"].max()\n",
    "    plt_min = df_sales.loc[df_range.index, \"sales_uds\"].min()\n",
    "    plt.scatter(r, plt_min)\n",
    "    plt.scatter(r, plt_max)\n",
    "    \n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "plt.title(\"Max and min values in sales_currency\")\n",
    "plt.xlabel(\"Number of time the std\")\n",
    "for r in np.arange(0.5, 4, 0.5):\n",
    "    df_range = df_zscore.loc[((df_zscore[1] > -r*limBase[1]) & (df_zscore[1] < r*limBase[1]))]\n",
    "    plt_max = df_sales.loc[df_range.index, \"sales_currency\"].max()\n",
    "    plt_min = df_sales.loc[df_range.index, \"sales_currency\"].min()\n",
    "    plt.scatter(r, plt_min)\n",
    "    plt.scatter(r, plt_max)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify the analysis we will only take into account the values in USD and check how reasonable are the max sale values.  \n",
    "Note: that in the second plot the points values could use different currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1\n",
    "df_range = df_zscore.loc[((df_zscore[0] > -r*limBase[0]) & (df_zscore[0] < r*limBase[0]))]\n",
    "df_sales_merged.loc[df_range.index].sort_values(\"sales_currency\").tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While with r=0.5 the maximum values for sales_uds seem to align to a reasonable price for the time in which the sale took place, by increasing the range we get prices that easily double for ram that is specwise half as good (ie ~8000$ for a 8gb stick of ddr4 in 2018).  \n",
    "This analysis is not ideal since it doesn't take account of the currency exchange nor of the fluctuations in the supply of ram at the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "stdTimes = 0.5\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "plt.title(\"Dataset with outliers\")\n",
    "plt.scatter(df_zscore[0], df_zscore[1], color=\"g\", marker='*', label='Standard')\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "plt.title(\"Dataset without outliers\")\n",
    "df_zscore_wo_out = df_zscore.loc[((df_zscore[0] > -stdTimes*df_zscore[0].std()) & (df_zscore[0] < stdTimes*df_zscore[0].std()))]\n",
    "#df_zscore_wo_out = df_zscore.loc[((df_zscore[1] > -stdTimes*df_zscore[1].std()) & (df_zscore[1] < stdTimes*df_zscore[1].std()))]\n",
    "plt.scatter(df_zscore_wo_out[0], df_zscore_wo_out[1], color=\"g\", marker='*', label='Standard')\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_zscore[\"Outlier\"] = 0\n",
    "df_zscore.loc[~df_zscore.index.isin(df_zscore_wo_out.index), \"Outlier\"] = 1\n",
    "df_zscore_outlier_labels = df_zscore[\"Outlier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zscore_out = df_zscore.loc[~df_zscore.index.isin(df_zscore_wo_out.index)]\n",
    "df_zscore_out.shape[0] #Outliers removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are too many entries to be able to simply run something like DBSCAN on all of them to detect the noise.  \n",
    "We will therefore use BIRCH to perform a data reduction and then apply DBSCAN in order to approximate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates to speed up clustering/training\n",
    "cluster_out_sample = df_sales.drop_duplicates(subset=[\"sales_uds\", \"sales_currency\"])[[\"sales_uds\", \"sales_currency\"]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(cluster_out_sample)\n",
    "norm_out_sample = scaler.transform(cluster_out_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "brc = Birch(n_clusters=None, threshold=0.003)\n",
    "brc.fit(norm_out_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "centers_brc_df = pd.DataFrame(brc.subcluster_centers_)\n",
    "print(f\"Number of CF nodes as a result of Birch: {len(brc.subcluster_labels_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_brc = df_sales.copy() #Used to identify all outliers (even duplicates)\n",
    "\n",
    "sales_sample = df_sales[[\"sales_uds\", \"sales_currency\"]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(sales_sample)\n",
    "norm_out_brc_sample = scaler.transform(sales_sample)\n",
    "\n",
    "df_sales_brc[\"BIRCH\"] = brc.predict(norm_out_brc_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply DBSCAN on the BIRCH-produced representative centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples = 100, eps = 0.3)\n",
    "dbscan.fit(brc.subcluster_centers_)\n",
    "\n",
    "cluster_series = pd.Series(data=dbscan.labels_, index=centers_brc_df.index)\n",
    "cluster_outliers = cluster_series.transform(lambda x: 1 if x == -1 else 0)\n",
    "centers_brc_df[\"Outlier\"] = cluster_outliers\n",
    "\n",
    "outlier_brc_df = centers_brc_df.loc[centers_brc_df[\"Outlier\"] == 1]\n",
    "centers_brc_df_wo_out = centers_brc_df.loc[centers_brc_df[\"Outlier\"] == 0]\n",
    "\n",
    "outlier_count = list(dbscan.labels_).count(-1)\n",
    "print(f\"Number of outliers (BIRCH): {outlier_count}\")\n",
    "print(f\"Percentage of outliers among the data points: {outlier_count/centers_brc_df.shape[0]*100:3.2f}%\")\n",
    "print(f\"Silhouette score: {silhouette_score(centers_brc_df, dbscan.labels_)}\")\n",
    "print(f\"Number of clusters: {len(np.unique(dbscan.labels_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10)) \n",
    "fig_dims = (2, 3)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "plt.title(\"Sampled dataset with outliers\")\n",
    "init_axes = \\\n",
    "    plt.scatter(centers_brc_df[0], centers_brc_df[1], \n",
    "                color=\"g\", marker='*', label='Standard').axes\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "plt.title(\"Sampled dataset without outliers\")\n",
    "plt.scatter(centers_brc_df_wo_out[0], centers_brc_df_wo_out[1], color=\"g\")\n",
    "#Keep the same axis size for all the subplots\n",
    "plt.xlim(min(init_axes.viewLim.intervalx), max(init_axes.viewLim.intervalx))\n",
    "plt.ylim(min(init_axes.viewLim.intervaly), max(init_axes.viewLim.intervaly))\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 2))\n",
    "plt.title(\"Outliers\")\n",
    "plt.scatter(outlier_brc_df[0], outlier_brc_df[1], color=\"r\")\n",
    "plt.xlim(min(init_axes.viewLim.intervalx), max(init_axes.viewLim.intervalx))\n",
    "plt.ylim(min(init_axes.viewLim.intervaly), max(init_axes.viewLim.intervaly))\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (1, 1))\n",
    "plt.title(\"Sampled dataset without outliers (Zoomed in)\")\n",
    "init_axes2 = plt.scatter(centers_brc_df_wo_out[0], centers_brc_df_wo_out[1], color=\"g\").axes\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (1, 0))\n",
    "plt.title(\"Sampled dataset with outliers (Zoomed in)\")\n",
    "plt.scatter(centers_brc_df_wo_out[0], centers_brc_df_wo_out[1], color=\"g\")\n",
    "plt.scatter(outlier_brc_df[0], outlier_brc_df[1], color=\"r\")\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 250)\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (1, 2))\n",
    "plt.title(\"Outliers (Zoomed in)\")\n",
    "plt.scatter(outlier_brc_df[0], outlier_brc_df[1], color=\"r\")\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 250)\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clustering_outlier_df = df_sales_brc.loc[df_sales_brc[\"BIRCH\"].isin(outlier_brc_df.index)].drop([\"BIRCH\"], axis=1)\n",
    "cluster_df_wo_out = df_sales_brc.loc[~df_sales_brc[\"BIRCH\"].isin(outlier_brc_df.index)].drop([\"BIRCH\"], axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 3)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "plt.title(\"Sampled dataset with outliers\")\n",
    "init_axes = \\\n",
    "    plt.scatter(df_sales_brc[\"sales_uds\"], df_sales_brc[\"sales_currency\"], \n",
    "                color=\"g\", marker='*', label='Standard').axes\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "plt.title(\"Sampled dataset without outliers\")\n",
    "plt.scatter(cluster_df_wo_out[\"sales_uds\"], cluster_df_wo_out[\"sales_currency\"], color=\"g\", marker='*', label='Standard')\n",
    "#Keep the same axis size for all the subplots\n",
    "plt.xlim(min(init_axes.viewLim.intervalx), max(init_axes.viewLim.intervalx))\n",
    "plt.ylim(min(init_axes.viewLim.intervaly), max(init_axes.viewLim.intervaly))\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 2))\n",
    "plt.title(\"Outliers\")\n",
    "plt.scatter(clustering_outlier_df[\"sales_uds\"], clustering_outlier_df[\"sales_currency\"], color=\"r\", marker='*', label='Standard')\n",
    "plt.xlim(min(init_axes.viewLim.intervalx), max(init_axes.viewLim.intervalx))\n",
    "plt.ylim(min(init_axes.viewLim.intervaly), max(init_axes.viewLim.intervaly))\n",
    "plt.xlabel(\"Sales (USD)\")\n",
    "plt.ylabel(\"Sales (Local currency)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering_wo_out = cluster_df_wo_out\n",
    "df_clustering_outlier_labels = centers_brc_df[\"Outlier\"]\n",
    "\n",
    "print(f\"Number of outliers in the dataset: {clustering_outlier_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose the best type of outlier detection depending on the results of the feature analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vendor features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(dest_df, source_df):\n",
    "\n",
    "    #Modify the data frame locally (possibly not needed)\n",
    "    source_df = source_df.copy()\n",
    "    \n",
    "    #Total number of items sold by vendor\n",
    "    IFeature = source_df.groupby([\"vendor_code\"]).size().rename(\"I\")\n",
    "    dest_df = dest_df.join(IFeature, on=\"vendor_code\")\n",
    "    \n",
    "    #Total number of unique items sold by vendor\n",
    "    IuFeature = source_df.groupby([\"vendor_code\"]).ram_code.nunique()\n",
    "    dest_df = dest_df.join(IuFeature, on=\"vendor_code\").rename(columns={\"ram_code\":\"Iu\"})\n",
    "    \n",
    "    #Max value of item sold (USD)\n",
    "    MaxValFeature = source_df.groupby([\"vendor_code\"]).apply(lambda x: x.sales_uds.max()).rename(\"MaxValuePerOrder\")\n",
    "    dest_df = dest_df.join(MaxValFeature, on=\"vendor_code\")\n",
    "    \n",
    "    #Avg value per item sold (USD)\n",
    "    AvgValFeature = source_df.groupby([\"vendor_code\"]).apply(lambda x: x.sales_uds.sum()/x.shape[0]).rename(\"AvgValuePerOrder\")\n",
    "    dest_df = dest_df.join(AvgValFeature, on=\"vendor_code\")\n",
    "    \n",
    "    #Max value items sold in a month (USD)\n",
    "    vendorRamCount = source_df.groupby([\"vendor_code\", \"year\", \"month\"])[\"sales_uds\"].sum()\n",
    "    IMaxValMonthFeature = vendorRamCount.groupby([\"vendor_code\"]).max().rename(\"IMaxMonthSales\")\n",
    "    dest_df = dest_df.join(IMaxValMonthFeature, on=\"vendor_code\")\n",
    "    \n",
    "    #Avg value items sold in a month (USD)\n",
    "    vendorRamCount = source_df.groupby([\"vendor_code\", \"year\", \"month\"])[\"sales_uds\"].sum()\n",
    "    IAvgValMonthFeature = vendorRamCount.groupby([\"vendor_code\"]).mean().rename(\"IAvgMonthSales\")\n",
    "    dest_df = dest_df.join(IAvgValMonthFeature, on=\"vendor_code\")\n",
    "    \n",
    "    #Max number items sold in a month\n",
    "    vendorRamCount = source_df.groupby([\"vendor_code\", \"year\", \"month\"]).size()\n",
    "    IMaxMonthFeature = vendorRamCount.groupby([\"vendor_code\"]).max().rename(\"IMaxMonthItems\")\n",
    "    dest_df = dest_df.join(IMaxMonthFeature, on=\"vendor_code\")\n",
    "    \n",
    "    #Avg number items sold in a month\n",
    "    vendorRamCount = source_df.groupby([\"vendor_code\", \"year\", \"month\"]).size()\n",
    "    IAvgMonthFeature = vendorRamCount.groupby([\"vendor_code\"]).mean().rename(\"IAvgMonthItems\")\n",
    "    dest_df = dest_df.join(IAvgMonthFeature, on=\"vendor_code\")\n",
    "    \n",
    "    #Number of months in business\n",
    "    vendorMonthCount = source_df.groupby([\"vendor_code\", \"year\"]).apply(lambda x: x.month.nunique())\n",
    "    TotMonthFeature = vendorMonthCount.groupby([\"vendor_code\"]).sum().rename(\"TotMonthBusiness\")\n",
    "    dest_df = dest_df.join(TotMonthFeature, on=\"vendor_code\")\n",
    "    \n",
    "    #The Shannon entropy on the selling behaviour of the vendor: countries of operation\n",
    "    probSeriesGrouped = source_df.groupby([\"vendor_code\"])\\\n",
    "        .apply(lambda x: x.groupby([\"geo_code\"]).size()/x.size)\n",
    "    logSeriesGrouped = np.log2(probSeriesGrouped)\n",
    "    ProdProbLogSeriesGrouped = -1 * probSeriesGrouped * logSeriesGrouped\n",
    "    EFeature = ProdProbLogSeriesGrouped.groupby([\"vendor_code\"]).sum()\n",
    "    EFeature.name = \"Egeo\"\n",
    "    dest_df = dest_df.join(EFeature, on=\"vendor_code\")\n",
    "\n",
    "    return dest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes regarding the RAM features  \n",
    "We deliberately didn't include any features defined on the characteristics of the RAM sold.  \n",
    "In the context of a vendor dataset we care about the value of the items sold and therefore we assume that the market is more capable of weighting the importance of the RAM features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the features to the datasets with and without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_vendor_id = df_sales[\"vendor_code\"].sort_values().unique()\n",
    "vend_df = pd.DataFrame(data=unq_vendor_id, columns=[\"vendor_code\"]).set_index(\"vendor_code\")\n",
    "\n",
    "df_sales_wo_out_zscore = df_sales_merged.loc[df_zscore_wo_out.index]\n",
    "df_sales_wo_out_clustering = df_sales_merged.loc[df_clustering_wo_out.index]\n",
    "\n",
    "#Add the features to the empty dataframe\n",
    "vend_df_w_out = add_features(vend_df, df_sales_merged) #Dataframe containing customer features with outliers\n",
    "vend_df_wo_out_zscore = add_features(vend_df, df_sales_wo_out_zscore) #Dataframe containing customer features without outliers (zscore)\n",
    "vend_df_wo_out_clustering = add_features(vend_df, df_sales_wo_out_clustering) #Dataframe containing customer features without outliers (clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature analysis  \n",
    "Comparison between feature dataset obtained from the vendor dataset with and without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vend_df_w_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_w_out = vend_df_w_out.corr()\n",
    "corr_w_out_vis = corr_w_out.copy()\n",
    "threshold = 0.7\n",
    "\n",
    "for col in corr_w_out.columns:\n",
    "    corr_w_out_vis[col] = corr_w_out_vis[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "    \n",
    "corr_w_out_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_wo_out_zscore = vend_df_wo_out_zscore.corr()\n",
    "corr_wo_out_zscore_vis = corr_wo_out_zscore.copy()\n",
    "threshold = 0.7\n",
    "\n",
    "for col in corr_wo_out_zscore.columns:\n",
    "    corr_wo_out_zscore_vis[col] = corr_wo_out_zscore_vis[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "    \n",
    "corr_wo_out_zscore_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_wo_out_clustering = vend_df_wo_out_clustering.corr()\n",
    "corr_wo_out_clustering_vis = corr_wo_out_clustering.copy()\n",
    "threshold = 0.7\n",
    "\n",
    "for col in corr_wo_out_clustering.columns:\n",
    "    corr_wo_out_clustering_vis[col] = corr_wo_out_clustering_vis[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "    \n",
    "corr_wo_out_clustering_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between the correlation matrixes of the two approaches\n",
    "diff_corr_outliers = corr_wo_out_zscore-corr_wo_out_clustering\n",
    "\n",
    "threshold = 0.1\n",
    "\n",
    "for col in diff_corr_outliers.columns:\n",
    "    diff_corr_outliers[col] = diff_corr_outliers[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "    \n",
    "diff_corr_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between features don't change a whole lot when applying either type of technique to remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between the correlation matrixes of the dataset with and without outliers\n",
    "diff_corr_outliers = corr_w_out-corr_wo_out_clustering\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "for col in diff_corr_outliers.columns:\n",
    "    diff_corr_outliers[col] = diff_corr_outliers[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "    \n",
    "diff_corr_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only significant effect that removing the outliers has is regarding some \"Max\" features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With outliers\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(vend_df_w_out.values)\n",
    "norm_out_sample = scaler.transform(vend_df_w_out.values)\n",
    "\n",
    "#Add name column temporarily\n",
    "tmp_vendor_df = pd.DataFrame(norm_out_sample, columns=vend_df_w_out.columns)\n",
    "tmp_vendor_df = tmp_vendor_df.join(df_vendor[\"name\"])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "pd.plotting.parallel_coordinates(tmp_vendor_df, \"name\")\n",
    "plt.gca().get_legend().remove() #Remove legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without outliers (zscore)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(vend_df_wo_out_zscore.values)\n",
    "norm_out_sample = scaler.transform(vend_df_wo_out_zscore.values)\n",
    "\n",
    "#Add name column temporarily\n",
    "tmp_vendor_df = pd.DataFrame(norm_out_sample, columns=vend_df_wo_out_zscore.columns)\n",
    "tmp_vendor_df = tmp_vendor_df.join(df_vendor[\"name\"])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "pd.plotting.parallel_coordinates(tmp_vendor_df, \"name\")\n",
    "plt.gca().get_legend().remove() #Remove legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without outliers (clustering)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(vend_df_wo_out_clustering.values)\n",
    "norm_out_sample = scaler.transform(vend_df_wo_out_clustering.values)\n",
    "\n",
    "#Add name column temporarily\n",
    "tmp_vendor_df = pd.DataFrame(norm_out_sample, columns=vend_df_wo_out_clustering.columns)\n",
    "tmp_vendor_df = tmp_vendor_df.join(df_vendor[\"name\"])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "pd.plotting.parallel_coordinates(tmp_vendor_df, \"name\")\n",
    "plt.gca().get_legend().remove() #Remove legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different outlier detection approaches didn't differ much with respect to their impact on the feature dataset as it can be seen in the above results.  \n",
    "From now on we will take into account only the clustering approach since it gave about the same results while removing less data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vend_df_wo_out = vend_df_wo_out_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature redundancy  \n",
    "For our analysis purposes, we don't need all of the features defined above given the high correlations between them.  \n",
    "We will therefore remove these redundant features (corr > 0.90) in order to simplify future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vend_df_w_out_redund = vend_df_w_out.drop([\"IMaxMonthSales\", \"IAvgMonthSales\", \"IMaxMonthItems\", \"IAvgMonthItems\"], axis=1)\n",
    "vend_df_wo_out_redund = vend_df_wo_out.drop([\"IMaxMonthSales\", \"IAvgMonthSales\", \"IMaxMonthItems\", \"IAvgMonthItems\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature outliers  \n",
    "Analysis of the impact on the feature outliers of keeping or not the outliers in the vendor dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we first normalise the data in order to apply PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(vend_df_w_out_redund)\n",
    "norm_data_w_out = scaler.transform(vend_df_w_out_redund)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(vend_df_wo_out_redund)\n",
    "norm_data_wo_out = scaler.transform(vend_df_wo_out_redund)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(norm_data_w_out)\n",
    "print(f\"Variance explained by PCA components: {pca.explained_variance_ratio_.sum()}\")\n",
    "pca_df_w_out = pd.DataFrame(X_pca, index=vend_df_w_out.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(norm_data_wo_out)\n",
    "print(f\"Variance explained by PCA components: {pca.explained_variance_ratio_.sum()}\")\n",
    "pca_df_wo_out = pd.DataFrame(X_pca, index=vend_df_wo_out.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#With outliers in the initial dataset\n",
    "outlier_detection = DBSCAN(min_samples = 3, eps = 1.4)\n",
    "clusters = outlier_detection.fit_predict(pca_df_w_out)\n",
    "outlier_count = list(clusters).count(-1)\n",
    "print(f\"Number of outliers among the data points: {outlier_count}\")\n",
    "print(f\"Percentage of outliers among the data points: {outlier_count/vend_df_w_out.shape[0]*100:3.2f}%\")\n",
    "print(f\"Silhouette score: {silhouette_score(pca_df_w_out, outlier_detection.labels_)}\")\n",
    "print(f\"Number of clusters: {len(np.unique(outlier_detection.labels_))}\")\n",
    "\n",
    "cluster_series = pd.Series(data=clusters, index=pca_df_w_out.index)\n",
    "cluster_outliers = cluster_series.transform(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "pca_df_w_out[\"Outlier\"] = cluster_outliers\n",
    "color_list_w_out = pca_df_w_out[\"Outlier\"].map(lambda x: \"r\" if x == 1 else \"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without outliers in the initial dataset\n",
    "outlier_detection = DBSCAN(min_samples = 3, eps = 1.7)\n",
    "clusters = outlier_detection.fit_predict(pca_df_wo_out)\n",
    "outlier_count = list(clusters).count(-1)\n",
    "print(f\"Number of outliers among the data points: {outlier_count}\")\n",
    "print(f\"Percentage of outliers among the data points: {outlier_count/vend_df_wo_out.shape[0]*100:3.2f}%\")\n",
    "print(f\"Silhouette score: {silhouette_score(pca_df_wo_out, outlier_detection.labels_)}\")\n",
    "print(f\"Number of clusters: {len(np.unique(outlier_detection.labels_))}\")\n",
    "\n",
    "cluster_series = pd.Series(data=clusters, index=pca_df_wo_out.index)\n",
    "cluster_outliers = cluster_series.transform(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "pca_df_wo_out[\"Outlier\"] = cluster_outliers\n",
    "color_list_wo_out = pca_df_wo_out[\"Outlier\"].map(lambda x: \"r\" if x == 1 else \"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plot_3d_scatter([pca_df_w_out, pca_df_wo_out], [color_list_w_out, color_list_wo_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_index_w_out = pca_df_w_out.loc[pca_df_w_out[\"Outlier\"] == 1].index\n",
    "out_df_w_out = vend_df_w_out_redund.loc[out_index_w_out]\n",
    "\n",
    "out_df_w_out.join(df_vendor[\"name\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_index_wo_out = pca_df_wo_out.loc[pca_df_wo_out[\"Outlier\"] == 1].index\n",
    "out_df_wo_out = vend_df_wo_out_redund.loc[out_index_wo_out]\n",
    "\n",
    "out_df_wo_out.join(df_vendor[\"name\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_outlier_df.loc[clustering_outlier_df[\"vendor_code\"] == 68]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the outliers in the inital dataset doesn't seem to have much of an impact on the outliers of the feature dataset, the problematic vendors are the same for the most part.  \n",
    "We prefer however to continue our analysis on the vendor dataset that is built on the sales data without outliers since in some instances, like with the vendor RamCity above, the results can be greatly skewed.  \n",
    "  \n",
    "We will continue the analysis with two dataframes: one including the five vendors classified as outliers and one excluding them.  \n",
    "Both of these datasets have been built using the sales data without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df_index = pca_df_wo_out.loc[pca_df_wo_out[\"Outlier\"] == 0].index\n",
    "\n",
    "feature_df_w_out = vend_df_wo_out_redund\n",
    "feature_df_wo_out = vend_df_wo_out_redund.loc[feature_df_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature analysis  \n",
    "Comparison between the vendor dataset with and without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df_w_out.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df_wo_out.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With outliers\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feature_df_w_out.values)\n",
    "norm_out_sample = scaler.transform(feature_df_w_out.values)\n",
    "\n",
    "#Add name column temporarily\n",
    "tmp_vendor_df = pd.DataFrame(norm_out_sample, columns=feature_df_w_out.columns)\n",
    "tmp_vendor_df = tmp_vendor_df.join(df_vendor[\"name\"])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Vendor dataset (with outliers)\")\n",
    "pd.plotting.parallel_coordinates(tmp_vendor_df, \"name\")\n",
    "plt.gca().get_legend().remove() #Remove legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without outliers\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feature_df_wo_out.values)\n",
    "norm_out_sample = scaler.transform(feature_df_wo_out.values)\n",
    "\n",
    "#Add name column temporarily\n",
    "tmp_vendor_df = pd.DataFrame(norm_out_sample, columns=feature_df_wo_out.columns)\n",
    "tmp_vendor_df = tmp_vendor_df.join(df_vendor[\"name\"])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Vendor dataset (without outliers)\")\n",
    "pd.plotting.parallel_coordinates(tmp_vendor_df, \"name\")\n",
    "plt.gca().get_legend().remove() #Remove legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_w_out = feature_df_w_out.corr()\n",
    "corr_w_out_vis = corr_w_out.copy()\n",
    "threshold = 0.5\n",
    "\n",
    "for col in corr_w_out.columns:\n",
    "    corr_w_out_vis[col] = corr_w_out_vis[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "    \n",
    "corr_w_out_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_wo_out = feature_df_wo_out.corr()\n",
    "corr_wo_out_vis = corr_wo_out.copy()\n",
    "threshold = 0.5\n",
    "\n",
    "for col in corr_wo_out.columns:\n",
    "    corr_wo_out_vis[col] = corr_wo_out_vis[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "    \n",
    "corr_wo_out_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff_corr = corr_wo_out-corr_w_out\n",
    "\n",
    "threshold = 0.1\n",
    "\n",
    "for col in diff_corr.columns:\n",
    "    diff_corr[col] = diff_corr[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "    \n",
    "diff_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions  \n",
    "The two datasets seem to be quite similar, so for now we will keep both of them and compare the results obtained from the clustering and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df_w_out.to_csv('./task1-result_w_out.csv', sep=',', index=True)\n",
    "feature_df_wo_out.to_csv('./task1-result_wo_out.csv', sep=',', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO  \n",
    "- Fix duplication in ram dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
