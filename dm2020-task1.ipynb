{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1: Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for displaying plots below the cell\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('customer_supermarket.csv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to contain data about the shopping habits of the customers of a grocery store chain.  \n",
    "Each row represents an object purchased:  \n",
    "- BasketID: identifies a batch of items bought during the same shopping session  \n",
    "- BasketDate: date in which the shopping session took place  \n",
    "- Sale: represents the value of the item, we need to figure out if it refers to a single item or the item*quantity\n",
    "- CustomerID: identifies a unique customer\n",
    "- CustomerCountry: represents the country in which the purchase took place\n",
    "- ProdID: identifies a unique product for sale\n",
    "- ProdDescr: describes the product\n",
    "- Qta: number of items bought with id ProdID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only ProdDescr and CustomerID contain null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics regarding the CustomerID are meaningless since the assignment of an ID is usually done progressively and without having any additional information on the customer.  \n",
    "We need to fix the data type situation in order to get a better understanding of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type conversion  \n",
    "Let's start by checking out the data type that pandas assigns to the attributes, in order to get an idea of the potential problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.convert_dtypes().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomerID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomerID got converted to a reasonable data type while the others became a generic \"string\".  \n",
    "However there is no point in having CustomerID as an int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CustomerID\"] = df[\"CustomerID\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasketDate\n",
    "Let's convert the BasketDate type from String to datetime, just in case we need to perform some analysis that requires ordinal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BasketDate = pd.to_datetime(df.BasketDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Sale\" attribute is considered a generic object while it should be recognised as a float.  \n",
    "Let's see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sale.map(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.Sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Sale uses a comma instead of a point to separate the decimal part, so it is considered a \"str\" instead of a \"float64\".  \n",
    "Let's replace the commas in \"Sale\" with dots in order to have them be recognised as float64 by pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sale = df.Sale.apply(lambda x: x.replace(',','.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sale = df.Sale.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sale is now correctly identified as a float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration data frame\n",
    "Used for exploration purposes but not necessarily useful for clustering.  \n",
    "Initialised with some additional features that could prove useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auxiliary df to be used throughout the data understanding phase\n",
    "df_expl = df.copy()\n",
    "\n",
    "df_expl[\"QtaPositive\"] = 0\n",
    "df_expl.loc[df_expl[\"Qta\"] > 0, \"QtaPositive\"] = 1 #Indicates whether the records Qta is positive\n",
    "\n",
    "df_expl[\"SalePositive\"] = 0\n",
    "df_expl.loc[df_expl[\"Sale\"] > 0, \"SalePositive\"] = 1 #Indicates whether the records Sale is positive\n",
    "\n",
    "df_expl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasketID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of unique CustomerID is: {len(df['CustomerID'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the different types of BasketID\n",
    "Let's check why BasketID is not considered an int64 like CustomerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonNumSeries = pd.to_numeric(df.BasketID, errors='coerce').isnull()\n",
    "# Print the records with BasketIDs containing a non-numeric value\n",
    "df[nonNumSeries].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[nonNumSeries, \"BasketID\"].str.slice(0,1).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a good chunk of the BasketID values start with a \"C\" and some with \"A\" instead of being just numbers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_c_df = df.loc[df[\"BasketID\"].str.get(0) == \"C\"]\n",
    "len_basket_c = len(basket_c_df)\n",
    "print(f\"Records starting with 'C' (Size: {len_basket_c}):\\n\")\n",
    "basket_c_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_a_df = df.loc[df[\"BasketID\"].str.get(0) == \"A\"]\n",
    "len_basket_a = len(basket_a_df)\n",
    "print(f\"Records starting with 'A' (Size: {len_basket_a}):\\n\")\n",
    "basket_a_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a strong correlation between the \"C\" and a negative quantity, this could indicate a customer that asked for a refund.  \n",
    "\n",
    "There is also some interesting correlation between the \"A\" start and a ProdDescr containing \"Adjust bad debt\", maybe the \"A\" stands for adjust and since the CustomerID in both cases is NaN this could be an operation that concerns only the management of the shop and not something that concerns the customers (which is our primary objective).  \n",
    "These records, however, are too few to be meaningful, they skew too much the characteristics of the sale data (outliers) and they don't concern the activities of the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to add a \"BasketID type A\" and \"BasketID type C\" binary attribute (0/1) and see if there are correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise all the cells to 0\n",
    "df_expl[\"BasketIDTypeA\"] = 0\n",
    "df_expl[\"BasketIDTypeC\"] = 0\n",
    "df_expl[\"BasketIDTypeStd\"] = 0\n",
    "\n",
    "#Set the cells appropriately depending on the BasketID type\n",
    "df_expl.loc[df_expl[\"BasketID\"].str.get(0) == \"A\", \"BasketIDTypeA\"] = 1\n",
    "df_expl.loc[df_expl[\"BasketID\"].str.get(0) == \"C\", \"BasketIDTypeC\"] = 1\n",
    "df_expl.loc[(df_expl[\"BasketIDTypeA\"] == 0) & (df_expl[\"BasketIDTypeC\"] == 0), \"BasketIDTypeStd\"] = 1\n",
    "\n",
    "df_expl[\"NewBasketID\"] = df_expl[\"BasketID\"]\n",
    "\n",
    "#Remove the initial letter from BasketID where necessary\n",
    "df_expl.loc[df_expl[\"BasketID\"].str.get(0) == \"A\", \"NewBasketID\"] = df_expl.loc[(df_expl[\"BasketID\"].str.get(0) == \"A\"), \"BasketID\"].str.slice(start=1)\n",
    "df_expl.loc[df_expl[\"BasketID\"].str.get(0) == \"C\", \"NewBasketID\"] = df_expl.loc[(df_expl[\"BasketID\"].str.get(0) == \"C\"), \"BasketID\"].str.slice(start=1)\n",
    "\n",
    "df_expl.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BasketID of type C has a strong negative correlation with the sign of Qta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.loc[df[\"BasketID\"].str.get(0) == \"C\", \"ProdDescr\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could this mean for the C type? Probably indicates discounts/refunds, further checks necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"NewBasketID\"] = df_expl[\"NewBasketID\"].astype(\"int64\")\n",
    "df_expl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there are no more anomalies inside BasketID since it can be now converted to int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"NewBasketID\"] = df_expl[\"NewBasketID\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we now have less unique BasketIDs in our records, after removing the letter that identifies the type from the BasketID attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The original number of unique BasketIDs is: {df_expl[\"BasketID\"].unique().size}')\n",
    "print(f'The current number of unique BasketIDs is: {df_expl[\"NewBasketID\"].unique().size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number is the same, therefore each BasketID of type A or C didn't merge with pre-existing shopping sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the merger of type C records into standard records\n",
    "Let's check if it is possible/reasonable to associate each type C record with a standard record with the same ProdID, Sale and CustomerID.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"ProdSaleQta\"] = df_expl[\"Qta\"]*df_expl[\"Sale\"]\n",
    "df_expl[\"AbsProdSaleQta\"] = df_expl[\"ProdSaleQta\"].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only about 9000 entries to check, computationally feasible\n",
    "df_typec = df_expl.loc[df_expl[\"BasketIDTypeC\"] == 1].copy()\n",
    "std_df = df_expl.loc[df_expl[\"BasketIDTypeStd\"] == 1].copy()\n",
    "\n",
    "#New dataframe that we are going to save and reuse later for further analysis\n",
    "type_c_id_list = df_typec[\"BasketID\"].sort_values().unique()\n",
    "refunds_dict = {}\n",
    "\n",
    "non_refundable_df = pd.DataFrame(columns=std_df.columns)\n",
    "n_refunded_requests = 0\n",
    "\n",
    "i = 0\n",
    "#Increase sample size for a more accurate analysis\n",
    "#Set to df_typec.shape[0] for a complete check (Warning: takes a while)\n",
    "sampleSize = 10\n",
    "\n",
    "for index, series in df_typec.iterrows():\n",
    "    \n",
    "    #Constraints\n",
    "    tmp_df1 = std_df.loc[(std_df[\"BasketDate\"] <= series[\"BasketDate\"]) & \n",
    "                         (std_df[\"Sale\"] == series[\"Sale\"]) & \n",
    "                         (std_df[\"CustomerID\"] == series[\"CustomerID\"]) & \n",
    "                         (std_df[\"ProdID\"] == series[\"ProdID\"])]\n",
    "    \n",
    "    tmp_df1 = tmp_df1.sort_values([\"BasketDate\"], ascending=False)\n",
    "    \n",
    "    #Stronger requirement, if present otherwise merge with a record that satisfies the requirements of tmp_df1\n",
    "    tmp_df2 = tmp_df1.loc[(tmp_df1[\"AbsProdSaleQta\"] == series[\"AbsProdSaleQta\"])]\n",
    "    #Whenever in doubt about the API properties...\n",
    "    tmp_df2 = tmp_df2.sort_values([\"BasketDate\"], ascending=False)\n",
    "    \n",
    "    order_found = None\n",
    "\n",
    "    if(tmp_df2.empty == False):\n",
    "        #Given the additional constraint the first entry is sufficient\n",
    "        order_found = tmp_df2.iloc[0]\n",
    "\n",
    "        std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"AbsProdSaleQta\"] = 0\n",
    "        std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"Qta\"] = 0\n",
    "        \n",
    "        #Save result\n",
    "        #print(order_found[\"BasketID\"])\n",
    "        refunds_dict[(series[\"BasketID\"], series[\"ProdID\"])] = pd.array([order_found['BasketID']])\n",
    "        \n",
    "    elif(tmp_df1.empty == False):\n",
    "        order_list = []\n",
    "        amount_to_refund = -1*series[\"Qta\"]\n",
    "        tmp_i = 0\n",
    "        \n",
    "        while(amount_to_refund > 0 and len(tmp_df1.index) > tmp_i):\n",
    "            order_found = tmp_df1.iloc[tmp_i]\n",
    "            tmp_i = tmp_i + 1\n",
    "            \n",
    "            if(order_found[\"Qta\"] == 0):\n",
    "                continue\n",
    "            \n",
    "            order_list.append(order_found[\"BasketID\"])\n",
    "            amount_to_refund = amount_to_refund - order_found[\"Qta\"]\n",
    "            \n",
    "            #Need to avoid two different refunds referencing the same order without any items left to be refunded\n",
    "            if(amount_to_refund >= 0):\n",
    "                tmp_df1.loc[tmp_df1[\"BasketID\"] == order_found[\"BasketID\"], \"Qta\"] = 0\n",
    "                std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"Qta\"] = 0\n",
    "            else:\n",
    "                tmp_df1.loc[tmp_df1[\"BasketID\"] == order_found[\"BasketID\"], \"Qta\"] = -1*amount_to_refund\n",
    "                std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"Qta\"] = -1*amount_to_refund\n",
    "                amount_to_refund = 0\n",
    "        \n",
    "        #Save result\n",
    "        #print(order_list)\n",
    "        if(amount_to_refund == 0):\n",
    "            n_refunded_requests = n_refunded_requests + 1\n",
    "    else:\n",
    "        #print(f\"No match found\")\n",
    "        #print(series)\n",
    "        #print(std_df.loc[(std_df[\"CustomerID\"] == series[\"CustomerID\"]) & \n",
    "        #                 (std_df[\"Sale\"] == series[\"Sale\"]) & \n",
    "        #                 (std_df[\"ProdID\"] == series[\"ProdID\"])].head())\n",
    "        \n",
    "        non_refundable_df = non_refundable_df.append(series)\n",
    "        \n",
    "    #Avoids a long computation on the full dataset\n",
    "    if(i >= sampleSize):\n",
    "        break\n",
    "    \n",
    "    #print(i)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of entries without a corresponding purchase: {len(non_refundable_df.index)/sampleSize*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result calculated on the whole subset type C is : 34.58%\n",
    "\n",
    "Considering the amount of refund entries for which there is no corresponding purchase there seems to be some missing data or our interpretation of type C records is incorrect.  \n",
    "For this reason we decide to consider type C records as a generic cost, relative to a specific CustomerID, incurred by the supermarket.  \n",
    "The other entries with negative Qta will be considered as costs that did not originate from the interaction with a particular customer.  \n",
    "  \n",
    "The sum of the entries with negative Qta will henceforth be considered as the TotalCost paid by the company while the sum of everything else is the Revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the shopping habits of a customer that returned for a \"refund\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_cond = [\"CustomerID\", pd.Grouper(key=\"BasketDate\", freq=\"D\")]\n",
    "\n",
    "refund_all_cust_df = df_expl.groupby(grouping_cond).filter(lambda x: (x[\"BasketIDTypeC\"] == 1).any())\n",
    "\n",
    "refund_and_shop_cust_df = df_expl.groupby(grouping_cond)\\\n",
    "            .filter(lambda x: (x[\"BasketIDTypeC\"] == 1).any() and (x[\"BasketIDTypeC\"] == 0).any())\n",
    "\n",
    "refund_only_cust_df = df_expl.groupby(grouping_cond).filter(lambda x: (x[\"BasketIDTypeC\"] == 1).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refund_all_value = refund_all_cust_df.groupby(grouping_cond).ngroups\n",
    "refund_and_shop_value = refund_and_shop_cust_df.groupby(grouping_cond).ngroups\n",
    "\n",
    "print(f\"The percentage of times that a customer buys items in the same day when he/she refunds something is: \" + \\\n",
    "        f\"{refund_and_shop_value/refund_all_value*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasketDate\n",
    "Let's see how the entries are distributed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = math.ceil(math.log(len(df[\"BasketID\"].unique()), 2) + 1) #Sturge's rule\n",
    "\n",
    "\n",
    "#Since there is only one date for each BasketID take only the first element in each group\n",
    "df_expl.loc[df_expl[\"BasketIDTypeStd\"] == 1]\\\n",
    "    .groupby([\"BasketID\"]).nth(0)[\"BasketDate\"].hist(bins=k, figsize=(10,5), alpha=0.5)\n",
    "\n",
    "df_expl.loc[df_expl[\"BasketIDTypeC\"] == 1].groupby([\"BasketID\"]).nth(0)[\"BasketDate\"].hist(bins=k, figsize=(10,5), alpha=0.5)\n",
    "\n",
    "df_expl.loc[(df_expl[\"BasketIDTypeStd\"] == 1) & (df_expl[\"Qta\"] < 0)]\\\n",
    "    .groupby([\"BasketID\"]).nth(0)[\"BasketDate\"].hist(bins=k, figsize=(10,5), alpha=0.5)\n",
    "\n",
    "\n",
    "plt.legend(('Standard (All)', 'Type C', 'Standard (Qta < 0)'), loc='best')\n",
    "plt.title('Distribution of BasketIDs with respect to months')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of transactions increases month by month.  \n",
    "Possibly reflects a change of policy regarding data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_dict = dict(zip([0,1,2,3,4,5,6], [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]))\n",
    "\n",
    "basketday = df.groupby([\"BasketID\"]).nth(0)[\"BasketDate\"].dt.weekday #Series containing the day of the week of each BasketID\n",
    "basketday = basketday.sort_values().transform(lambda x: days_dict[x])\n",
    "basketday.hist(bins=np.arange(0, len(days_dict)+1, 1), figsize=(10,5))\n",
    "plt.title('Distribution of BasketIDs with respect to weekday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saturday and sunday seem to be the least active days for the shops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_dict = dict(zip([1,2,3,4,5,6, 7, 8, 9, 10, 11, 12], \n",
    "                       [\"January\", \"February\", \"March\", \"April\", \n",
    "                        \"May\", \"June\", \"July\", \"August\", \n",
    "                        \"September\", \"October\", \"November\", \"December\"]))\n",
    "\n",
    "basketmonth = df.groupby(by=[\"BasketID\"]).nth(0)[\"BasketDate\"].dt.month #Series containing the day of the week of each BasketID\n",
    "basketmonth = basketmonth.sort_values().transform(lambda x: months_dict[x]) #Sort needed to have the correct ordering in the plot\n",
    "basketmonth.hist(bins=np.arange(0, len(months_dict)+1, 1), figsize=(15,5))\n",
    "plt.title('Distribution of BasketIDs with respect to month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sales peak happens in November and the least amount of sales occurs in December."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distributions of Sale and Qta taking into account the BasketDate\n",
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "\n",
    "plt.scatter(df_expl[\"BasketDate\"], df_expl[\"Sale\"], color='g', marker='*', label='Standard')\n",
    "\n",
    "plt.xlabel('BasketDate')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Sale')\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "\n",
    "plt.scatter(df_expl[\"BasketDate\"], df_expl[\"Qta\"], color='g', marker='*', label='Standard')\n",
    "\n",
    "plt.xlabel('BasketDate')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Qta')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are clearly visible data points that could be classified as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = math.ceil(math.log(len(df[\"Sale\"].unique()), 2) + 1) #Sturge's rule\n",
    "\n",
    "df_expl[\"Sale\"].hist(bins=np.arange(0,k), figsize=(10,5))\n",
    "plt.xlabel(\"Sale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"Sale\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding negative Sale entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[df_expl[\"Sale\"] < 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only entries with a negative Sale are the type A entries, of which there are only two in the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the relationship between Sale and Qta\n",
    "We need to figure out if the Sale value refers to the cost of a single item or cost of item * Qta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"ProdID\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at this view of the Data Frame it seems that Sale doesn't change if the Qta changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no significant correlation between Sale and Qta, we can therefore assume that Sale is the cost of the single item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding Sale values equal to 0  \n",
    "In the context of sale 0 values don't make sense and therefore should be considered as missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"SaleNull\"] = 0\n",
    "df_expl.loc[df_expl[\"Sale\"] == 0, \"SaleNull\"] = 1\n",
    "n_missing_sale = df_expl.loc[df_expl[\"Sale\"] == 0].shape[0]\n",
    "n_tot_sale = df[\"Sale\"].shape[0]\n",
    "print(f\"Number of entries with Sale equal to 0: {n_missing_sale}\")\n",
    "print(f\"Percentage of entries with Sale equal to 0: {(n_missing_sale/n_tot_sale)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small percentage, relative to the entire dataset, of the entries presents a Sale value equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_expl.loc[df_expl[\"SaleNull\"] == 1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "df_expl.loc[df_expl[\"SaleNull\"] == 1, \"BasketDate\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be any apparent pattern to the entries with Sale equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomerID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding null CustomerIDs\n",
    "Let's see why the number of non-null CustomerID entries is so low and if there are any interesting properties to be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_expl.loc[df_expl[\"CustomerID\"].isna()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 65080 records with a null CustomerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"CustomerIDNull\"] = 0\n",
    "df_expl.loc[df_expl[\"CustomerID\"].isna(), \"CustomerIDNull\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.corr()[\"CustomerIDNull\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No interesting correlation.  \n",
    "Let's check if we can retrieve some missing CustomerIDs by using the records referencing the same BasketID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.groupby(by=\"BasketID\").filter(lambda x: x[\"CustomerID\"].isna().any() & x[\"CustomerID\"].notna().any()).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no intersection between records with the same BasketID but different CustomerIDNull value.  \n",
    "We can start to assume that the entries with CustomerID null are done on purpouse and have a specific meaning.  \n",
    "Further analysis in the Qta section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the relationship between CustomerID and Sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.loc[df_expl[\"CustomerIDNull\"] == 1].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant negative correlation between SaleNull and QtaPositive in the subset of records with missing CustomerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_expl.loc[(df_expl[\"CustomerIDNull\"] == 1) & (df_expl[\"Sale\"] == 0)].shape[0])\n",
    "print(df_expl.loc[(df_expl[\"CustomerIDNull\"] != 1) & (df_expl[\"Sale\"] == 0)].shape[0])\n",
    "print(df_expl.loc[(df_expl[\"CustomerIDNull\"] == 1) & (df_expl[\"Sale\"] != 0)].shape[0])\n",
    "print(df_expl.loc[(df_expl[\"CustomerIDNull\"] != 1) & (df_expl[\"Sale\"] != 0)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only one BasketID considered independently from the number of single entries\n",
    "basketid_country = df_expl.groupby(by=[\"CustomerCountry\"])[\"BasketID\"].nunique()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "\n",
    "basketid_country.plot(kind='bar')\n",
    "plt.title(\"Number of unique BasketIDs by country with UK\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "\n",
    "basketid_country.loc[basketid_country.index != \"United Kingdom\"].plot(kind='bar')\n",
    "plt.title(\"Number of unique BasketIDs by country without UK\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the singular operations take place in the United Kingdom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding missing Sale values by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_by_country_df = df_expl.loc[df_expl[\"SaleNull\"] == 1].groupby(by=[\"CustomerCountry\"])[\"BasketID\"].nunique()\n",
    "missing_by_country_df.plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_by_country_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be problematic to do data segmentation with respect to the country of a missing value (TODO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProdID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "df[\"ProdID\"].value_counts().plot()\n",
    "plt.xlabel(\"ProdID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ProdID interpretation  \n",
    "Let's find out why this wasn't converted to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"ProdID\"].str.isnumeric(), (\"ProdID\", \"ProdDescr\")].value_counts() #Records with ProdIDs containing only numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"ProdID\"].str.isalpha(), (\"ProdID\", \"ProdDescr\")].value_counts() #Records with ProdIDs containing only letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Records with ProdID terminating with a letter\n",
    "term_letter_prodid = df.loc[(df[\"ProdID\"].str.slice(start=-1).str.isalpha()) & (df[\"ProdID\"].str.slice(0, -1).str.isnumeric())]\n",
    "term_letter_prodid[[\"ProdID\", \"ProdDescr\"]].sort_values(by=\"ProdID\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presence of letters seems to indicate different variations of the same item.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the possibility of having multiple entries for the same (BasketID, ProdID) entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_df = df_expl.groupby([\"BasketID\", \"ProdID\"]).filter(lambda x: x.shape[0] > 1)\n",
    "ambiguous_df.sort_values([\"BasketID\", \"ProdID\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ambiguous_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuple (BasketID, ProdID) is, counterintuitively, not a \"Key\" of the dataset.  \n",
    "It shouldn't matter too much for the purpouse of our analysis but it could prove tricky in other situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding ProdIDs in type C BasketID records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.loc[(df_expl[\"BasketIDTypeC\"] == 1)].groupby([\"ProdID\"]).apply(lambda x: x[\"Qta\"].sum()).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most returned products by quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.loc[(df_expl[\"BasketIDTypeC\"] == 1)].groupby([\"ProdID\"]).apply(lambda x: x[\"ProdSaleQta\"].sum()).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most returned products by total sale value.  \n",
    "The presence of items such as AMAZONFEE and M (manual adjustments) seem to further justify our choice of not considering each type C entry as connected with a standard type one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = math.ceil(math.log(len(df[\"Qta\"].unique()), 2) + 1) #Sturge's rule\n",
    "df[\"Qta\"].hist(bins=np.arange(-k, k))\n",
    "plt.xlabel(\"Qta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Qta\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df[\"Qta\"] == 0].size)\n",
    "print(df.loc[df[\"Qta\"].isna()].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no records with Qta equal to 0 or NaN, so we can assume that there are no records with missing values in the feature Qta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding negative Qta and type C BasketID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.corr()[\"QtaPositive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the BasketID section there is a strong correlation between the sign of Qta and a BasketID of type C.  \n",
    "Let's see if there is some interesting distribution in the remaining negative quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_not_c = df_expl.loc[(df_expl[\"Qta\"] < 0) & (df_expl[\"BasketIDTypeC\"] == 0)]\n",
    "neg_not_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the trend of Sale equal 0 continues throughout the subset of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_not_c[\"Sale\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does.  \n",
    "Let's check if all CustomerIDs in the subset are Null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_not_c.describe()[\"CustomerIDNull\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are all Null, as can be deduced by the min value.  \n",
    "All entries not of type C and with a negative Qta have a Sale value equal to 0 and a CustomerID null.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the intersection between ProdID entries with positive and negative Qtas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_neg = set(df.loc[df[\"Qta\"] < 0, \"ProdID\"])\n",
    "set_pos = set(df.loc[df[\"Qta\"] > 0, \"ProdID\"])\n",
    "inter_list = list(set_pos & set_neg) #Intersection\n",
    "\n",
    "print(len(inter_list)/len(set_pos))\n",
    "print(len(inter_list)/len(set_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_neg = set(df.loc[(df[\"Qta\"] < 0) & (df[\"CustomerID\"].notna()), \"ProdID\"])\n",
    "set_pos = set(df.loc[(df[\"Qta\"] > 0) & (df[\"CustomerID\"].notna()), \"ProdID\"])\n",
    "inter_list = list(set_pos & set_neg)\n",
    "print(len(inter_list)/len(set_pos))\n",
    "print(len(inter_list)/len(set_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases there are some values of ProdID present in a particular subset of the dataset that are not present in the other.  \n",
    "Result needed to justify decision in customer features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = df.copy() #Just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = df.loc[df[\"Sale\"] == 0]\n",
    "print(f\"Number of records with a missing Sale value: {missing_df.shape[0]}\")\n",
    "print(f\"Number of unique ProdID of records with a missing Sale value: {len(missing_df['ProdID'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_df = df.copy()\n",
    "fill_df.loc[fill_df[\"Sale\"] == 0, \"Sale\"] = np.NaN\n",
    "\n",
    "fill_group = fill_df.groupby([\"ProdID\"])\\\n",
    "        .apply(lambda x: x[\"Sale\"].fillna(x.loc[x[\"Sale\"].notna(), \"Sale\"].median()))\n",
    "\n",
    "fill_df[\"Sale\"] = fill_group.droplevel(\"ProdID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Data Segmentation to separate the dataset into different groups depending on the ProdID and then we used the median of the non-null Sale entries to fill the missing values.  \n",
    "We used the median instead of the mean in order to be less susceptible to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_missing_df = fill_df.loc[fill_df['Sale'].isna()]\n",
    "print(f\"Number of records with missing Sale value after fill: {still_missing_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small percentage of the entries cannot have its missing value replaced using data segmentation since the only entries with those particular ProdIDs lack a non-null Sale value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.loc[still_missing_df.index].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these entries are missing a CustomerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fill_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleanup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(still_missing_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all entries still without a Sale value in order to allow for clustering.  \n",
    "There shouldn't be a problem with this decision, given that we are removing about 70 records from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Pointless for now since the customer features consider only CustomerID non-null entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove type A BasketID entries since, as noted above, they don't specify a CustomerID and are concerned with the grocery store chain debt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(df.loc[df[\"BasketID\"].str.get(0) == \"A\"].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove records with negative Qta that do not reference a type C BasketID since they do not reference any CustomerID.  \n",
    "Now all records with negative Qta are of type C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(df.loc[(df[\"BasketID\"].str.get(0) != \"C\") & (df[\"Qta\"] < 0)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't seem to have a way to retrieve the CustomerID in case it is missing let's delete all entries without CustomerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(df.loc[df[\"CustomerID\"].isna()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier analysis\n",
    "Outliers in this dataset are expected to be found in the numerical attributes, therefore we will focus on Sale and Qta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"Sale\"], df[\"Qta\"], color='g', marker='*', label='Standard')\n",
    "plt.xlabel(\"Sale\")\n",
    "plt.ylabel(\"Qta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical approach\n",
    "As seen in the respective sections the data doesn't seem to have a normal distribution.  \n",
    "They do however seem to have an exponential one. (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "right_whisker = Q3 + 1.5*IQR\n",
    "left_whisker = Q1 - 1.5*IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Qta 2.5-quantile: {df['Qta'].quantile(0.025)}\")\n",
    "print(f\"Qta 97.5-quantile: {df['Qta'].quantile(0.975)}\")\n",
    "print(f\"Sale 2.5-quantile: {df['Sale'].quantile(0.025)}\")\n",
    "print(f\"Sale 97.5-quantile: {df['Sale'].quantile(0.975)}\\n\")\n",
    "\n",
    "print(f\"Left whisker: {left_whisker}\")\n",
    "print(f\"Right whisker: {right_whisker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems, from the scatter plot and the quantiles, that by using the IQR approach we could lose interesting information that doesn't diverge all that much from the rest, both visually and numerically, for example we would lose any Sale value greater then 12.75 while we can see that there are many data points with a greater value that are not that different (ie 99-quantile of Sale = 16.95).  \n",
    "\n",
    "Note also that by removing the outliers in Qta with the IQR approach we will also remove all entries having a negative Qta.  \n",
    "We need to consider if that data is worth keeping.  \n",
    "The same concerns do not apply to Sale, as seen in the Sale section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_out_iqr = df.copy()\n",
    "\n",
    "df_wo_out_iqr = df_wo_out_iqr.loc[(df_wo_out_iqr[\"Sale\"] >= left_whisker[\"Sale\"]) & (df_wo_out_iqr[\"Sale\"] <= right_whisker[\"Sale\"])]\n",
    "df_wo_out_iqr = df_wo_out_iqr.loc[(df_wo_out_iqr[\"Qta\"] >= left_whisker[\"Qta\"]) & (df_wo_out_iqr[\"Qta\"] <= right_whisker[\"Qta\"])]\n",
    "\n",
    "print(f'The number of total outliers in the dataset is: {df.shape[0]-df_wo_out_iqr.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach we eliminate a significant amount of all the records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0])\n",
    "print(df.drop_duplicates(subset=[\"Sale\", \"Qta\"]).shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drastically reduce the number of elements to apply the clustering algorithm on by just removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = df.drop_duplicates(subset=[\"Sale\", \"Qta\"]).copy()\n",
    "cluster_sample = cluster_df[[\"Sale\", \"Qta\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(cluster_sample)\n",
    "norm_sample = scaler.transform(cluster_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first normalize the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "outlier_detection = DBSCAN(min_samples = 3, eps = 0.4) #4.3\n",
    "clusters = outlier_detection.fit_predict(norm_sample)\n",
    "print(f\"Number of outliers among the unique data points: {list(clusters).count(-1)}\")\n",
    "\n",
    "cluster_series = pd.Series(data=clusters, index=cluster_df.index)\n",
    "cluster_outliers = cluster_series.transform(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "cluster_df[\"Outlier\"] = cluster_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = cluster_df.loc[cluster_df[\"Outlier\"] == 1]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "plt.title(\"Dataset with outliers\")\n",
    "plt.scatter(df[\"Sale\"], df[\"Qta\"], color=\"g\", marker='*', label='Standard')\n",
    "plt.xlabel(\"Sale\")\n",
    "plt.ylabel(\"Qta\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "plt.title(\"Outliers\")\n",
    "plt.scatter(outlier_df[\"Sale\"], outlier_df[\"Qta\"], color=\"g\", marker='*', label='Standard')\n",
    "plt.xlabel(\"Sale\")\n",
    "plt.ylabel(\"Qta\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach's result more closely matches our expectations of which data points are outliers.  \n",
    "(TODO: could be useful to also take into account BasketDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_tuples = outlier_df[[\"Sale\", \"Qta\"]].to_numpy()\n",
    "#A list consisting of the outlier couples (Sale,Qta)\n",
    "outlier_list = list(zip(outlier_df[\"Sale\"].to_numpy(), outlier_df[\"Qta\"].to_numpy()))\n",
    "\n",
    "df_wo_out_clustering = \\\n",
    "    df.groupby([\"Sale\", \"Qta\"]).filter(lambda x: not((x[\"Sale\"].to_numpy()[0], x[\"Qta\"].to_numpy()[0]) in outlier_list)).copy()\n",
    "\n",
    "print(f'The number of total outliers in the dataset is: {df.shape[0]-df_wo_out_clustering.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all of the data points in which the couple (\"Sale\", \"Qta\") is included in the list of couples that identify outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "Using the Clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_out = df_wo_out_clustering\n",
    "df_w_out = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep both dataframes (with and without outliers) to compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some new features to be used for customer analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(dest_df, source_df):\n",
    "\n",
    "#Required features\n",
    "\n",
    "    #Modify the data frame locally\n",
    "    source_df = source_df.copy()\n",
    "    \n",
    "    #Additional useful data for features\n",
    "    source_df[\"DateYear\"] = source_df[\"BasketDate\"].dt.year\n",
    "    source_df[\"DateMonth\"] = source_df[\"BasketDate\"].dt.month\n",
    "    source_df[\"DateDay\"] = source_df[\"BasketDate\"].dt.day\n",
    "    source_df[\"DateWeekDay\"] = source_df[\"BasketDate\"].dt.weekday\n",
    "    \n",
    "    #Split the dataset into positive Qta and negative Qta in order to simplify notation\n",
    "    #Note 1: CustomerID null are filtered out by groupby.\n",
    "    pos_df = source_df.loc[source_df[\"Qta\"] > 0]\n",
    "    neg_df = source_df.loc[source_df[\"Qta\"] < 0]\n",
    "    \n",
    "    #Total number of items bought by customer\n",
    "    # Since we cannot create a clear association between the refunds and purchases,\n",
    "    #  we decided to not include the negative Qta entries (type C) in this feature.\n",
    "    # The negative Qtas will be considered in a separate feature. \n",
    "    IFeature = pos_df.groupby([\"CustomerID\"]).Qta.sum()\n",
    "    dest_df = dest_df.merge(IFeature, on=\"CustomerID\").rename(columns={\"Qta\":\"I\"})\n",
    "\n",
    "    #Total number of unique items bought by customer\n",
    "    #As seen in the Qta section, we need to limit the counting to the positive Qta entries,\n",
    "    # otherwise we also will count some entries for which there is no recorded purchase entry.\n",
    "    IuFeature = pos_df.groupby([\"CustomerID\"]).ProdID.nunique()\n",
    "    dest_df = dest_df.join(IuFeature, on=\"CustomerID\").rename(columns={\"ProdID\":\"Iu\"})\n",
    "\n",
    "    #Max number of item bought by customer across all shopping sessions\n",
    "    BasketIDQtaSum = pos_df.groupby([\"CustomerID\", \"BasketID\"]).Qta.sum()\n",
    "    ImaxFeature = BasketIDQtaSum.groupby([\"CustomerID\"]).max()\n",
    "    dest_df = dest_df.join(ImaxFeature, on=\"CustomerID\").rename(columns={\"Qta\":\"Imax\"})\n",
    "    \n",
    "    #The Shannon entropy on the purchasing behaviour of the customer: types of items bought\n",
    "    #Since we are assuming that the type C BasketID entries are a generic cost associated to a CustomerID,\n",
    "    # we can ignore them for the purpouse of computing the entropy since they might refer to a ProdID that cannot\n",
    "    # be bought Ie AMAZONFEE\n",
    "    \n",
    "    probSeriesGrouped = pos_df.groupby([\"CustomerID\"])\\\n",
    "            .apply(lambda x: x.groupby([\"ProdID\"])[\"Qta\"].sum()/x[\"Qta\"].sum())\n",
    "    logSeriesGrouped = np.log2(probSeriesGrouped)\n",
    "    ProdProbLogSeriesGrouped = -1 * probSeriesGrouped * logSeriesGrouped\n",
    "    EFeature = ProdProbLogSeriesGrouped.groupby([\"CustomerID\"]).sum()\n",
    "    EFeature = EFeature.fillna(0)\n",
    "    EFeature.name = \"Eproduct\"\n",
    "    dest_df = dest_df.join(EFeature, on=\"CustomerID\")\n",
    "\n",
    "#Additional features\n",
    "\n",
    "    #Average total revenue per month\n",
    "    #Revenue doesn't take into account the costs (negative Qta)\n",
    "    tot_revenue_cust = pos_df.groupby([\"CustomerID\"]).apply(lambda x: (x[\"Sale\"]*x[\"Qta\"]).sum())\n",
    "    n_month_cust = pos_df.groupby([\"CustomerID\"]).apply(lambda x: x.groupby([\"DateYear\", \"DateMonth\"]).ngroups)\n",
    "    AvgFeature = tot_revenue_cust/n_month_cust\n",
    "    AvgFeature.name = \"AvgRevenueMonth\"\n",
    "    dest_df = dest_df.join(AvgFeature, on=\"CustomerID\")\n",
    "    \n",
    "    #Average total costs per month\n",
    "    #Note: since the entries with costs are a small amount compared to all the others,\n",
    "    # some customers will have NaN as value. We will replace it with 0.\n",
    "    tot_costs_cust = neg_df.groupby([\"CustomerID\"]).apply(lambda x: (x[\"Sale\"]*x[\"Qta\"]).sum())\n",
    "    n_month_cust = neg_df.groupby([\"CustomerID\"]).apply(lambda x: x.groupby([\"DateYear\", \"DateMonth\"]).ngroups)\n",
    "    AvgFeature = (tot_costs_cust/n_month_cust).abs()\n",
    "    AvgFeature.name = \"AvgCostsMonth\"\n",
    "    dest_df = dest_df.join(AvgFeature, on=\"CustomerID\")\n",
    "    dest_df[\"AvgCostsMonth\"] = dest_df[\"AvgCostsMonth\"].fillna(0)\n",
    "\n",
    "    #Average customer shopping sessions per month\n",
    "    #We are interested in finding out about the habits of a customer and given the percentage\n",
    "    # of type C entries compared to standard ones it seems to us that costs are an unusual event\n",
    "    # rather than the norm. Therefore we exclude the type C entries from our definition of shopping session.\n",
    "    tot_sessions_cust = pos_df.groupby([\"CustomerID\"]).apply(lambda x: x[\"BasketID\"].nunique())\n",
    "    n_month_cust = pos_df.groupby([\"CustomerID\"]).apply(lambda x: x.groupby([\"DateYear\", \"DateMonth\"]).ngroups)\n",
    "    AvgFeature = tot_sessions_cust/n_month_cust\n",
    "    AvgFeature.name = \"AvgSessionsMonth\"\n",
    "    dest_df = dest_df.join(AvgFeature, on=\"CustomerID\")\n",
    "\n",
    "    #The Shannon entropy on the purchasing behaviour of the customer: days of the week of shopping\n",
    "    #Justification for type C entries same as for other entropy related feature.\n",
    "    probSeriesGrouped = pos_df.groupby([\"CustomerID\"])\\\n",
    "        .apply(lambda x: x.groupby([\"DateWeekDay\"])[\"BasketID\"].nunique()/x[\"BasketID\"].nunique())\n",
    "    logSeriesGrouped = np.log2(probSeriesGrouped)\n",
    "    ProdProbLogSeriesGrouped = -1 * probSeriesGrouped * logSeriesGrouped\n",
    "    EFeature = ProdProbLogSeriesGrouped.groupby([\"CustomerID\"]).sum()\n",
    "    EFeature.name = \"Eweekday\"\n",
    "    dest_df = dest_df.join(EFeature, on=\"CustomerID\")\n",
    "    \n",
    "    return dest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the new features to both the dataset with and without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_cust_id = df_w_out[\"CustomerID\"].sort_values().unique()[0:-1] #Remove NaN value, last value\n",
    "cust_df_w_out = pd.DataFrame(data=unq_cust_id, columns=[\"CustomerID\"]) #Dataframe containing customer features with outliers\n",
    "\n",
    "unq_cust_id = df_wo_out[\"CustomerID\"].sort_values().unique()[0:-1] #Remove NaN value, last value\n",
    "cust_df_wo_out = pd.DataFrame(data=unq_cust_id, columns=[\"CustomerID\"]) #Dataframe containing customer features without outliers\n",
    "\n",
    "cust_df_w_out[\"CustomerID\"] = cust_df_w_out[\"CustomerID\"].astype(\"object\")\n",
    "cust_df_wo_out[\"CustomerID\"] = cust_df_wo_out[\"CustomerID\"].astype(\"object\")\n",
    "\n",
    "cust_df_w_out = add_features(cust_df_w_out, df_w_out)\n",
    "cust_df_wo_out = add_features(cust_df_wo_out, df_wo_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df_w_out.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df_wo_out.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(cust_df_w_out, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(cust_df_wo_out, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough Eproduct seems to follow a normal distribution in both situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display correlations that have an absolute value greater than threshold\n",
    "corr_w_out = cust_df_w_out.corr()\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "for col in corr_w_out.columns:\n",
    "    corr_w_out[col] = corr_w_out[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "\n",
    "corr_w_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the outliers, we have a strong correlation between the maximum number of items bought in a single order by a customer and the average revenue per month.  \n",
    "Other potentially interesting correlations are the ones between the average revenue and average costs, and between Imax and average costs.  \n",
    "Note however that keeping these features with high correlations can be detrimental for the task of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display correlations that have an absolute value greater than threshold\n",
    "corr_wo_out = cust_df_wo_out.corr()\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "for col in corr_wo_out.columns:\n",
    "    corr_wo_out[col] = corr_wo_out[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "\n",
    "corr_wo_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there is a significant correlation between the max number of items bought and the average expenditure per month of the customer.  \n",
    "This could suggest to us that the customers with the biggest expenditures tend to buy more items per session (further analysis required). (TODO)  \n",
    "The other significant correlations don't seem particularly interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "diff_corr = (cust_df_w_out.corr() - cust_df_wo_out.corr()).abs()\n",
    "\n",
    "for col in diff_corr.columns:\n",
    "    diff_corr[col] = diff_corr[col].transform(lambda x: x if x>threshold else 0)\n",
    "\n",
    "print(\"Difference between the correlation matrices (with and without outliers)\")\n",
    "diff_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the Outlier analysis section, by using the dataset without outliers we lose almost all of the information concerning entries with negative Qta.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eproduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df_w_out[\"Eproduct\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize the attribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer-related outliers analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(cust_df_w_out.values)\n",
    "X = scaler.transform(cust_df_w_out.values)\n",
    "\n",
    "DBSCAN(eps=0.1, min_samples=2, algorithm='ball_tree').fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove customer related outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = cust_df.quantile(0.25)\n",
    "Q3 = cust_df.quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fig_height = cust_df.columns.size\n",
    "#fig = plt.figure(figsize=(20, 30)) \n",
    "#fig_dims = (fig_height, 2)\n",
    "#fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "#index = 0\n",
    "\n",
    "for feature in cust_df.columns:\n",
    "    #plt.subplot2grid(fig_dims, (index, 0))\n",
    "    #cust_df.boxplot(feature)\n",
    "\n",
    "    cust_df = cust_df.loc[(cust_df[feature] >= Q1[feature] - 1.5*IQR[feature]) & (cust_df[feature] <= Q3[feature] + 1.5*IQR[feature])]\n",
    "    \n",
    "    #plt.subplot2grid(fig_dims, (index, 1))\n",
    "    #cust_df.boxplot(feature)\n",
    "    #index = index + 1\n",
    "    \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(cust_df, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between Iu and Eweekday is even stronger after removing the outliers.  \n",
    "In order to reduce the dimensionality of the data we will drop Eweekday given the strong similarity to Iu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df = cust_df.drop(\"Eweekday\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task end\n",
    "Save the customer features for the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df_w_out.to_csv(\"customer_features.csv\", sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
