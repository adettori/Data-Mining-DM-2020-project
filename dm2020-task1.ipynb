{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1: Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for displaying plots below the cell\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('customer_supermarket.csv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to contain data about the shopping habits of the customers of a grocery store chain.  \n",
    "Each row represents an object purchased:  \n",
    "- BasketID: identifies a batch of items bought during the same shopping session  \n",
    "- BasketDate: date in which the shopping session took place  \n",
    "- Sale: represents the value of the item, we need to figure out if it refers to a single item or the item*quantity\n",
    "- CustomerID: identifies a unique customer\n",
    "- CustomerCountry: represents the country in which the purchase took place\n",
    "- ProdID: identifies a unique product for sale\n",
    "- ProdDescr: describes the product\n",
    "- Qta: number of items bought with id ProdID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only ProdDescr and CustomerID contain null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics regarding the CustomerID are meaningless since the assignment of an ID is usually done progressively and without having any additional information on the customer.  \n",
    "We need to fix the data type situation in order to get a better understanding of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type conversion  \n",
    "Let's start by checking out the data type that pandas assigns to the attributes, in order to get an idea of the potential problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.convert_dtypes().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomerID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomerID got converted to a reasonable data type while the others became a generic \"string\".  \n",
    "However there is no point in having CustomerID as an int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CustomerID\"] = df[\"CustomerID\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasketDate\n",
    "Let's convert the BasketDate type from String to datetime, just in case we need to perform some analysis that requires ordinal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BasketDate = pd.to_datetime(df.BasketDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Sale\" attribute is considered a generic object while it should be recognised as a float.  \n",
    "Let's see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sale.map(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.Sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Sale uses a comma instead of a point to separate the decimal part, so it is considered a \"str\" instead of a \"float64\".  \n",
    "Let's replace the commas in \"Sale\" with dots in order to have them be recognised as float64 by pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sale = df.Sale.apply(lambda x: x.replace(',','.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sale = df.Sale.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sale is now correctly identified as a float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration data frame\n",
    "Used for exploration purposes but not necessarily useful for clustering.  \n",
    "Initialised with some additional features that could prove useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auxiliary df to be used throughout the data understanding phase\n",
    "df_expl = df.copy()\n",
    "\n",
    "df_expl[\"QtaPositive\"] = 0\n",
    "df_expl.loc[df_expl[\"Qta\"] > 0, \"QtaPositive\"] = 1 #Indicates whether the records Qta is positive\n",
    "\n",
    "df_expl[\"SalePositive\"] = 0\n",
    "df_expl.loc[df_expl[\"Sale\"] > 0, \"SalePositive\"] = 1 #Indicates whether the records Sale is positive\n",
    "\n",
    "df_expl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasketID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the different types of BasketID\n",
    "Let's check why BasketID is not considered an int64 like CustomerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonNumSeries = pd.to_numeric(df.BasketID, errors='coerce').isnull()\n",
    "# Print the records with BasketIDs containing a non-numeric value\n",
    "df[nonNumSeries].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[nonNumSeries, \"BasketID\"].str.slice(0,1).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a good chunk of the BasketID values start with a \"C\" and some with \"A\" instead of being just numbers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_c_df = df.loc[df[\"BasketID\"].str.get(0) == \"C\"]\n",
    "len_basket_c = len(basket_c_df)\n",
    "print(f\"Records starting with 'C' (Size: {len_basket_c}):\\n\")\n",
    "basket_c_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_a_df = df.loc[df[\"BasketID\"].str.get(0) == \"A\"]\n",
    "len_basket_a = len(basket_a_df)\n",
    "print(f\"Records starting with 'A' (Size: {len_basket_a}):\\n\")\n",
    "basket_a_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a strong correlation between the \"C\" and a negative quantity, this could indicate a customer that asked for a refund.  \n",
    "\n",
    "There is also some interesting correlation between the \"A\" start and a ProdDescr containing \"Adjust bad debt\", maybe the \"A\" stands for adjust and since the CustomerID in both cases is NaN this could be an operation that concerns only the management of the shop and not something that concerns the customers (which is our primary objective).  \n",
    "These records, however, are too few to be meaningful, they skew too much the characteristics of the sale data (outliers) and they don't concern the activities of the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to add a \"BasketID type A\" and \"BasketID type C\" binary attribute (0/1) and see if there are correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise all the cells to 0\n",
    "df_expl[\"BasketIDTypeA\"] = 0\n",
    "df_expl[\"BasketIDTypeC\"] = 0\n",
    "df_expl[\"BasketIDTypeStd\"] = 0\n",
    "\n",
    "#Set the cells appropriately depending on the BasketID type\n",
    "df_expl.loc[df_expl[\"BasketID\"].str.get(0) == \"A\", \"BasketIDTypeA\"] = 1\n",
    "df_expl.loc[df_expl[\"BasketID\"].str.get(0) == \"C\", \"BasketIDTypeC\"] = 1\n",
    "df_expl.loc[(df_expl[\"BasketIDTypeA\"] == 0) & (df_expl[\"BasketIDTypeC\"] == 0), \"BasketIDTypeStd\"] = 1\n",
    "\n",
    "df_expl[\"NewBasketID\"] = df_expl[\"BasketID\"]\n",
    "\n",
    "#Remove the initial letter from BasketID where necessary\n",
    "df_expl.loc[df_expl[\"BasketID\"].str.get(0) == \"A\", \"NewBasketID\"] = df_expl.loc[(df_expl[\"BasketID\"].str.get(0) == \"A\"), \"BasketID\"].str.slice(start=1)\n",
    "df_expl.loc[df_expl[\"BasketID\"].str.get(0) == \"C\", \"NewBasketID\"] = df_expl.loc[(df_expl[\"BasketID\"].str.get(0) == \"C\"), \"BasketID\"].str.slice(start=1)\n",
    "\n",
    "df_expl.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BasketID of type C has a strong negative correlation with the sign of Qta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.loc[df[\"BasketID\"].str.get(0) == \"C\", \"ProdDescr\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could this mean for the C type? Probably indicates discounts/refunds, further checks necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"NewBasketID\"] = df_expl[\"NewBasketID\"].astype(\"int64\")\n",
    "df_expl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there are no more anomalies inside BasketID since it can be now converted to int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"NewBasketID\"] = df_expl[\"NewBasketID\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we now have less unique BasketIDs in our records, after removing the letter that identifies the type from the BasketID attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The original number of unique BasketIDs is: {df_expl[\"BasketID\"].unique().size}')\n",
    "print(f'The current number of unique BasketIDs is: {df_expl[\"NewBasketID\"].unique().size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number is the same, therefore each BasketID of type A or C didn't merge with pre-existing shopping sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the merger of type C records into standard records\n",
    "Let's check if it is possible/reasonable to associate each type C record with a standard record with the same ProdID, Sale and CustomerID.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"ProdSaleQta\"] = df_expl[\"Qta\"]*df_expl[\"Sale\"]\n",
    "df_expl[\"AbsProdSaleQta\"] = df_expl[\"ProdSaleQta\"].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only about 9000 entries to check, computationally feasible\n",
    "df_typec = df_expl.loc[df_expl[\"BasketIDTypeC\"] == 1].copy()\n",
    "std_df = df_expl.loc[df_expl[\"BasketIDTypeStd\"] == 1].copy()\n",
    "\n",
    "#New dataframe that we are going to save and reuse later for further analysis\n",
    "type_c_id_list = df_typec[\"BasketID\"].sort_values().unique()\n",
    "refunds_dict = {}\n",
    "\n",
    "non_refundable_df = pd.DataFrame(columns=std_df.columns)\n",
    "\n",
    "i = 0\n",
    "#Increase sample size for a more accurate analysis\n",
    "#Set to df_typec.shape[0] for a complete check (Warning: takes a while)\n",
    "sampleSize = 10\n",
    "\n",
    "for index, series in df_typec.iterrows():\n",
    "    tmp_df1 = std_df.loc[(std_df[\"BasketDate\"] <= series[\"BasketDate\"]) & \n",
    "                         (std_df[\"Sale\"] == series[\"Sale\"]) & \n",
    "                         (std_df[\"CustomerID\"] == series[\"CustomerID\"]) & \n",
    "                         (std_df[\"ProdID\"] == series[\"ProdID\"])]\n",
    "    \n",
    "    tmp_df1 = tmp_df1.sort_values([\"BasketDate\"], ascending=False)\n",
    "    \n",
    "    #Stronger requirement, if present otherwise merge with a record that satisfies the requirements of tmp_df1\n",
    "    tmp_df2 = tmp_df1.loc[(tmp_df1[\"AbsProdSaleQta\"] == series[\"AbsProdSaleQta\"])]\n",
    "    #Whenever in doubt about the API properties...\n",
    "    tmp_df2 = tmp_df2.sort_values([\"BasketDate\"], ascending=False)\n",
    "    \n",
    "    order_found = None\n",
    "\n",
    "    if(tmp_df2.empty == False):\n",
    "        #Given the additional constraint the first entry is sufficient\n",
    "        order_found = tmp_df2.iloc[0]\n",
    "\n",
    "        std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"AbsProdSaleQta\"] = 0\n",
    "        std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"Qta\"] = 0\n",
    "        \n",
    "        #Save result\n",
    "        #print(order_found[\"BasketID\"])\n",
    "        refunds_dict[(series[\"BasketID\"], series[\"ProdID\"])] = pd.array([order_found['BasketID']])\n",
    "        \n",
    "    elif(tmp_df1.empty == False):\n",
    "        order_list = []\n",
    "        amount_to_refund = -1*series[\"Qta\"]\n",
    "        tmp_i = 0\n",
    "        \n",
    "        while(amount_to_refund > 0 and len(tmp_df1.index) > tmp_i):\n",
    "            order_found = tmp_df1.iloc[tmp_i]\n",
    "            tmp_i = tmp_i + 1\n",
    "            \n",
    "            if(order_found[\"Qta\"] == 0):\n",
    "                continue\n",
    "            \n",
    "            order_list.append(order_found[\"BasketID\"])\n",
    "            amount_to_refund = amount_to_refund - order_found[\"Qta\"]\n",
    "            \n",
    "            #Need to avoid two different refunds referencing the same order without any items left to be refunded\n",
    "            if(amount_to_refund >= 0):\n",
    "                tmp_df1.loc[tmp_df1[\"BasketID\"] == order_found[\"BasketID\"], \"Qta\"] = 0\n",
    "                std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"Qta\"] = 0\n",
    "            else:\n",
    "                tmp_df1.loc[tmp_df1[\"BasketID\"] == order_found[\"BasketID\"], \"Qta\"] = -1*amount_to_refund\n",
    "                std_df.loc[(std_df[\"BasketID\"] == order_found[\"BasketID\"]) & \n",
    "                                (std_df[\"ProdID\"] == order_found[\"ProdID\"]), \"Qta\"] = -1*amount_to_refund\n",
    "        \n",
    "        #Save result\n",
    "        #print(order_list)\n",
    "        if(len(order_list) > 0):\n",
    "            refunds_dict[(series[\"BasketID\"], series[\"ProdID\"])] = pd.array(order_list)\n",
    "    else:\n",
    "        #print(f\"No match found\")\n",
    "        #print(series)\n",
    "        #print(std_df.loc[(std_df[\"CustomerID\"] == series[\"CustomerID\"]) & \n",
    "        #                 (std_df[\"Sale\"] == series[\"Sale\"]) & \n",
    "        #                 (std_df[\"ProdID\"] == series[\"ProdID\"])].head())\n",
    "        \n",
    "        non_refundable_df = non_refundable_df.append(series)\n",
    "        \n",
    "    #Avoids a long computation on the full dataset\n",
    "    if(i > sampleSize):\n",
    "        break\n",
    "    \n",
    "    #print(i)\n",
    "    i = i+1\n",
    "\n",
    "#Transform a python dictionary with a key of dimension=2 and one value for each key into a DataFrame with 3 columns\n",
    "#Avoids the problem of inserting lists with different lengths in the dataframe\n",
    "refund_df = pd.DataFrame(pd.Series(refunds_dict).reset_index()).set_axis(['RefundID', \"ProdID\",'OrigBasketID'],1,inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of entries without a corresponding purchase: {len(non_refundable_df.index)/sampleSize*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result calculated on the whole subset type C is : 34.58%\n",
    "\n",
    "Considering the amount of refund entries for which there is no corresponding purchase there seems to be some missing data or our interpretation of type C records is incorrect.  \n",
    "For this reason we decide to consider type C records as a generic cost, relative to a specific CustomerID, incurred by the supermarket.  \n",
    "The other entries with negative Qta will be considered as costs that did not originate from the interaction with a particular customer.  \n",
    "  \n",
    "The sum of the entries with negative Qta will henceforth be considered as the TotalCost paid by the company while the sum of everything else is the Revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the shopping habits of a customer that returned for a \"refund\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_cond = [\"CustomerID\", pd.Grouper(key=\"BasketDate\", freq=\"D\")]\n",
    "\n",
    "refund_all_cust_df = df_expl.groupby(grouping_cond).filter(lambda x: (x[\"BasketIDTypeC\"] == 1).any())\n",
    "\n",
    "refund_and_shop_cust_df = df_expl.groupby(grouping_cond)\\\n",
    "            .filter(lambda x: (x[\"BasketIDTypeC\"] == 1).any() and (x[\"BasketIDTypeC\"] == 0).any())\n",
    "\n",
    "refund_only_cust_df = df_expl.groupby(grouping_cond).filter(lambda x: (x[\"BasketIDTypeC\"] == 1).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refund_all_value = refund_all_cust_df.groupby(grouping_cond).ngroups\n",
    "refund_and_shop_value = refund_and_shop_cust_df.groupby(grouping_cond).ngroups\n",
    "\n",
    "print(f\"The percentage of times that a customer buys items in the same day when he/she refunds something is: \" + \\\n",
    "        f\"{refund_and_shop_value/refund_all_value*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasketDate\n",
    "Let's see how the entries are distributed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = math.ceil(math.log(len(df[\"BasketID\"].unique()), 2) + 1) #Sturge's rule\n",
    "\n",
    "\n",
    "#Since there is only one date for each BasketID take only the first element in each group\n",
    "df_expl.loc[df_expl[\"BasketIDTypeStd\"] == 1]\\\n",
    "    .groupby([\"BasketID\"]).nth(0)[\"BasketDate\"].hist(bins=k, figsize=(10,5), alpha=0.5)\n",
    "\n",
    "df_expl.loc[df_expl[\"BasketIDTypeC\"] == 1].groupby([\"BasketID\"]).nth(0)[\"BasketDate\"].hist(bins=k, figsize=(10,5), alpha=0.5)\n",
    "\n",
    "df_expl.loc[(df_expl[\"BasketIDTypeStd\"] == 1) & (df_expl[\"Qta\"] < 0)]\\\n",
    "    .groupby([\"BasketID\"]).nth(0)[\"BasketDate\"].hist(bins=k, figsize=(10,5), alpha=0.5)\n",
    "\n",
    "\n",
    "plt.legend(('Standard (All)', 'Type C', 'Standard (Qta < 0)'), loc='best')\n",
    "plt.title('Distribution of BasketIDs with respect to years')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of transactions increases month by month.  \n",
    "Possibly reflects a change of policy regarding data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_dict = dict(zip([0,1,2,3,4,5,6], [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]))\n",
    "\n",
    "basketday = df.groupby([\"BasketID\"]).nth(0)[\"BasketDate\"].dt.weekday #Series containing the day of the week of each BasketID\n",
    "basketday = basketday.sort_values().transform(lambda x: days_dict[x])\n",
    "basketday.hist(bins=np.arange(0, len(days_dict)+1, 1), figsize=(10,5))\n",
    "plt.title('Distribution of BasketIDs with respect to weekday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saturday and sunday seem to be the least active days for the shops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_dict = dict(zip([1,2,3,4,5,6, 7, 8, 9, 10, 11, 12], \n",
    "                       [\"January\", \"February\", \"March\", \"April\", \n",
    "                        \"May\", \"June\", \"July\", \"August\", \n",
    "                        \"September\", \"October\", \"November\", \"December\"]))\n",
    "\n",
    "basketmonth = df.groupby(by=[\"BasketID\"]).nth(0)[\"BasketDate\"].dt.month #Series containing the day of the week of each BasketID\n",
    "basketmonth = basketmonth.sort_values().transform(lambda x: months_dict[x]) #Sort needed to have the correct ordering in the plot\n",
    "basketmonth.hist(bins=np.arange(0, len(months_dict)+1, 1), figsize=(15,5))\n",
    "plt.title('Distribution of BasketIDs with respect to month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sales peak happens in November and the least amount of sales occurs in December."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distributions of Sale and Qta taking into account the BasketDate\n",
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "\n",
    "plt.scatter(df_expl[\"BasketDate\"], df_expl[\"Sale\"], color='g', marker='*', label='Standard')\n",
    "\n",
    "plt.xlabel('BasketDate')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Sale')\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "\n",
    "plt.scatter(df_expl[\"BasketDate\"], df_expl[\"Qta\"], color='g', marker='*', label='Standard')\n",
    "\n",
    "plt.xlabel('BasketDate')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Qta')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are clearly visible data points that could be classified as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"Sale\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding negative Sale entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[df_expl[\"Sale\"] < 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only entries with a negative Sale are the type A entries, of which there are only two in the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the relationship between Sale and Qta\n",
    "We need to figure out if the Sale value refers to the cost of a single item or cost of item * Qta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"ProdID\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at this view of the Data Frame it seems that Sale doesn't change if the Qta changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no significant correlation between Sale and Qta, we can therefore assume that Sale is the cost of the single item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the Sale distribution\n",
    "fig = plt.figure(figsize=(15, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "k = math.ceil(math.log(len(df[\"Sale\"].unique()), 2) + 1) #Sturge's rule\n",
    "df[\"Sale\"].hist(bins=np.arange(0, k, 1))\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "df.boxplot(column=[\"Sale\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the vast majority of Sale values (the price for a single item in a transaction) are small.  \n",
    "We need however to check for 0 values since they don't make sense in the contest of Sale and therefore should be considered as missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_missing_sale = df.loc[df[\"Sale\"] == 0, \"Sale\"].size\n",
    "n_tot_sale = df[\"Sale\"].size\n",
    "print(f\"Percentage of entries with Sale equal to 0: {(n_missing_sale/n_tot_sale)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small percentage, relative to the entire dataset, of the entries presents a Sale value equal to 0.  \n",
    "We will take care of them in the Data Preparation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomerID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding null CustomerIDs\n",
    "Let's see why the number of non-null CustomerID entries is so low and if there are any interesting properties to be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.loc[df_expl[\"CustomerID\"].isna()].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl[\"CustomerIDNull\"] = 0\n",
    "df_expl.loc[df_expl[\"CustomerID\"].isna(), \"CustomerIDNull\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.corr()[\"CustomerIDNull\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No interesting correlation.  \n",
    "Let's check if we can retrieve some missing CustomerIDs by using the records referencing the same BasketID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.groupby(by=\"BasketID\").filter(lambda x: x[\"CustomerID\"].isna().any() & x[\"CustomerID\"].notna().any()).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no intersection between records with the same BasketID but different CustomerIDNull value.  \n",
    "We can start to assume that the entries with CustomerID null are done on purpouse and have a specific meaning.  \n",
    "Further analysis in the Qta section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding entries with missing values in Sale and CustomerID  null\n",
    "Let's see if all the entries with Sale equal to 0 have also a missing CustomerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_sale_null_cust = df_expl.loc[(df_expl[\"CustomerIDNull\"] != 1) & (df_expl[\"Sale\"] == 0)]\n",
    "zero_sale_null_cust.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small amount of entries with both a Sale equal to 0 and a non-Null CustomerID.  \n",
    "(TODO?) (Check the percentage of Sale values present in other entries?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_sale_null_cust.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, as expected, the Qta is always positive for these entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only one BasketID considered independently from the number of single entries\n",
    "basketid_country = df_expl.groupby(by=[\"CustomerCountry\"])[\"BasketID\"].nunique()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5)) \n",
    "fig_dims = (1, 2)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 0))\n",
    "\n",
    "basketid_country.plot(kind='bar')\n",
    "plt.title(\"Number of unique BasketIDs by country with UK\")\n",
    "\n",
    "plt.subplot2grid(fig_dims, (0, 1))\n",
    "\n",
    "basketid_country.loc[basketid_country.index != \"United Kingdom\"].plot(kind='bar')\n",
    "plt.title(\"Number of unique BasketIDs by country without UK\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the singular operations take place in the United Kingdom.  \n",
    "Any further relevant analysis would require to solve the missing values problem first in order to have meaningful results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProdID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ProdID interpretation  \n",
    "Let's find out why this wasn't converted to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"ProdID\"].str.isnumeric(), (\"ProdID\", \"ProdDescr\")].value_counts() #Records with ProdIDs containing only numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"ProdID\"].str.isalpha(), (\"ProdID\", \"ProdDescr\")].value_counts() #Records with ProdIDs containing only letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Records with ProdID terminating with a letter\n",
    "term_letter_prodid = df.loc[(df[\"ProdID\"].str.slice(start=-1).str.isalpha()) & (df[\"ProdID\"].str.slice(0, -1).str.isnumeric())]\n",
    "term_letter_prodid[[\"ProdID\", \"ProdDescr\"]].sort_values(by=\"ProdID\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The letters seem to represent different variations of the same item.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the diversity and lack of structure of the ProdIDs, as can be seen in the different types listed above, there doesn't seem to be anything worth exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the possibility of having multiple entries for the same (BasketID, ProdID) entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_df = df_expl.groupby([\"BasketID\", \"ProdID\"]).filter(lambda x: x.shape[0] > 1)\n",
    "ambiguous_df.sort_values([\"BasketID\", \"ProdID\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ambiguous_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shouldn't matter too much for the purpouse of our analysis but it could prove tricky in other situations.  \n",
    "Note that for this reason the tuple (BasketID, ProdID) is not a \"Key\" of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding ProdIDs in type C BasketID records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.loc[(df_expl[\"BasketIDTypeC\"] == 1)].groupby([\"ProdID\"]).apply(lambda x: x[\"Qta\"].sum()).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most returned products by quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.loc[(df_expl[\"BasketIDTypeC\"] == 1)].groupby([\"ProdID\"]).apply(lambda x: x[\"ProdSaleQta\"].sum()).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most returned products by total sale value.  \n",
    "The presence of items such as AMAZONFEE and M (manual adjustments) seem to further justify our choice of not considering each type C entry as connected with a standard type one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Qta\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = math.ceil(math.log(len(df[\"Qta\"].unique()), 2) + 1) #Sturge's rule\n",
    "df[\"Qta\"].hist(bins=np.arange(0, k))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"Qta\"] == 0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no records with Qta equal to 0, so we can assume that there are no records with missing values in the feature Qta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding negative Qta and type C BasketID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expl.corr()[\"QtaPositive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the BasketID section there is a strong correlation between the sign of Qta and a BasketID of type C.  \n",
    "Let's see if there is some interesting distribution in the remaining negative quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_not_c = df_expl.loc[(df_expl[\"Qta\"] < 0) & (df_expl[\"BasketIDTypeC\"] == 0)]\n",
    "neg_not_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the trend of Sale equal 0 continues throughout the subset of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_not_c[\"Sale\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does.  \n",
    "Let's check if all CustomerIDs in the subset are Null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_not_c.describe()[\"CustomerIDNull\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are all Null, as can be deduced by the min value.  \n",
    "All entries not of type C and with a negative Qta have a Sale value equal to 0 and a CustomerID null.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the intersection between ProdID entries with positive and negative Qtas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_neg = set(df.loc[df[\"Qta\"] < 0, \"ProdID\"])\n",
    "set_pos = set(df.loc[df[\"Qta\"] > 0, \"ProdID\"])\n",
    "inter_list = list(set_pos & set_neg)\n",
    "print(len(inter_list)/len(set_pos))\n",
    "print(len(inter_list)/len(set_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_neg = set(df.loc[(df[\"Qta\"] < 0) & (df[\"CustomerID\"].notna()), \"ProdID\"])\n",
    "set_pos = set(df.loc[(df[\"Qta\"] > 0) & (df[\"CustomerID\"].notna()), \"ProdID\"])\n",
    "inter_list = list(set_pos & set_neg)\n",
    "print(len(inter_list)/len(set_pos))\n",
    "print(len(inter_list)/len(set_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases there are some values of ProdID present in a particular subset of the dataset that are not present in the other.  \n",
    "Result needed to justify decision in customer features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = df.copy() #Just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleanup   \n",
    "(Pointless for now since the customer features consider only CustomerID non-null entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove type A BasketID entries since, as noted above, they don't specify a CustomerID and are concerned with the grocery store chain debt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(df.loc[df[\"BasketID\"].str.get(0) == \"A\"].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove records with negative Qta that do not reference a type C BasketID since they do not reference any CustomerID.  \n",
    "Now all records with negative Qta are of type C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(df.loc[(df[\"BasketID\"].str.get(0) != \"C\") & (df[\"Qta\"] < 0)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't seem to have a way to retrieve the CustomerID in case it is missing let's delete all entries without CustomerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(df.loc[df[\"CustomerID\"].isna()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = df.loc[df[\"Sale\"] == 0]\n",
    "missing_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to retrieve the most accurate value possible for the missing Sale.  \n",
    "We will look for values present in the dataset that satisfy the maximum number of constraints. (TODO: test for better alternatives with models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "\n",
    "missed_df = pd.DataFrame(columns=missing_df.columns)\n",
    "\n",
    "for index, series in missing_df.iterrows():\n",
    "    \n",
    "    base_df = df.loc[df[\"ProdID\"] == series[\"ProdID\"]]\n",
    "    \n",
    "    #print(i)\n",
    "    i = i+1\n",
    "    \n",
    "    if(base_df.size == 0):\n",
    "        print(\"No match found\")\n",
    "        missed_df = missed_df.append(series)\n",
    "        continue\n",
    "    \n",
    "    base_res = base_df[\"Sale\"].median()\n",
    "    \n",
    "    lv1_df = base_df.loc[base_df[\"CustomerCountry\"] == series[\"CustomerCountry\"]]\n",
    "    lv1_res = lv1_df[\"Sale\"].median()\n",
    "    \n",
    "    if(lv1_df.size == 0):\n",
    "        df.loc[(df[\"BasketID\"] == series[\"BasketID\"]) & (df[\"ProdID\"] == series[\"ProdID\"]), \"Sale\"] = base_res\n",
    "        continue\n",
    "        \n",
    "    df.loc[(df[\"BasketID\"] == series[\"BasketID\"]) & (df[\"ProdID\"] == series[\"ProdID\"]), \"Sale\"] = lv1_res\n",
    "    \n",
    "    #TODO: add a BasketDate constraint?\n",
    "    #TODO: use K-nn?\n",
    "    \n",
    "print(f\"The number of entries for which a match wasn't found is: {missed_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values will be replaced by the median of the Sale values obtained by grouping with respect to the ProdID first and then, if possible, with respect to CustomerCountry.  \n",
    "We used the median instead of the mean in order to be less susceptible to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier analysis\n",
    "Outliers in this dataset are expected to be found in the numerical attributes, therefore we will focus on Sale and Qta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"Sale\"], df[\"Qta\"], color='g', marker='*', label='Standard')\n",
    "plt.xlabel(\"Sale\")\n",
    "plt.ylabel(\"Qta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical approach\n",
    "As seen in the respective sections the data doesn't seem to have a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Qta 2.5-quantile: {df['Qta'].quantile(0.025)}\")\n",
    "print(f\"Qta 97.5-quantile: {df['Qta'].quantile(0.975)}\")\n",
    "print(f\"Sale 2.5-quantile: {df['Sale'].quantile(0.025)}\")\n",
    "print(f\"Sale 97.5-quantile: {df['Sale'].quantile(0.975)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems, from the scatter plot and the quantiles, that by using the IQR approach we could lose interesting information that doesn't diverge all that much from the rest, both visually and numerically, for example we would lose any Sale value greater then 12.75 while we can see that there are many data points with a greater value that are not that different (ie 99-quantile of Sale = 16.95).  \n",
    "\n",
    "Note also that by removing the outliers in Qta with the IQR approach we will also remove all entries having a negative Qta.  \n",
    "We need to consider if that data is worth keeping.  \n",
    "The same concerns do not apply to Sale, as seen in the Sale section above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsuitable given the size of the dataset (considering DBSCAN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "Using the IQR method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_out = df.copy()\n",
    "\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the outliers in Sale and Qta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_out = df_wo_out.loc[(df_wo_out[\"Sale\"] >= Q1[\"Sale\"] - 1.5*IQR[\"Sale\"]) & (df_wo_out[\"Sale\"] <= Q3[\"Sale\"] + 1.5*IQR[\"Sale\"])]\n",
    "df_wo_out = df_wo_out.loc[(df_wo_out[\"Qta\"] >= Q1[\"Qta\"] - 1.5*IQR[\"Qta\"]) & (df_wo_out[\"Qta\"] <= Q3[\"Qta\"] + 1.5*IQR[\"Qta\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_out = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some new features to be used for customer analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(dest_df, source_df):\n",
    "\n",
    "#Required features\n",
    "\n",
    "    #Modify the data frame locally\n",
    "    source_df = source_df.copy()\n",
    "    \n",
    "    #Additional useful data for features\n",
    "    source_df[\"DateYear\"] = source_df[\"BasketDate\"].dt.year\n",
    "    source_df[\"DateMonth\"] = source_df[\"BasketDate\"].dt.month\n",
    "    source_df[\"DateDay\"] = source_df[\"BasketDate\"].dt.day\n",
    "    source_df[\"DateWeekDay\"] = source_df[\"BasketDate\"].dt.weekday\n",
    "    \n",
    "    #Split the dataset into positive Qta and negative Qta in order to simplify notation\n",
    "    #Note 1: CustomerID null are filtered out by groupby.\n",
    "    pos_df = source_df.loc[source_df[\"Qta\"] > 0]\n",
    "    neg_df = source_df.loc[source_df[\"Qta\"] < 0]\n",
    "    \n",
    "    #Total number of items bought by customer\n",
    "    # Since we cannot create a clear association between the refunds and purchases,\n",
    "    #  we decided to not include the negative Qta entries (type C) in this feature.\n",
    "    # The negative Qtas will be considered in a separate feature. \n",
    "    IFeature = pos_df.groupby([\"CustomerID\"]).Qta.sum()\n",
    "    dest_df = dest_df.merge(IFeature, on=\"CustomerID\").rename(columns={\"Qta\":\"I\"})\n",
    "\n",
    "    #Total number of unique items bought by customer\n",
    "    #As seen in the Qta section, we need to limit the counting to the positive Qta entries,\n",
    "    # otherwise we also will count some entries for which there is no recorded purchase entry.\n",
    "    IuFeature = pos_df.groupby([\"CustomerID\"]).ProdID.nunique()\n",
    "    dest_df = dest_df.join(IuFeature, on=\"CustomerID\").rename(columns={\"ProdID\":\"Iu\"})\n",
    "\n",
    "    #Max number of item bought by customer across all shopping sessions\n",
    "    BasketIDQtaSum = pos_df.groupby([\"CustomerID\", \"BasketID\"]).Qta.sum()\n",
    "    ImaxFeature = BasketIDQtaSum.groupby([\"CustomerID\"]).max()\n",
    "    dest_df = dest_df.join(ImaxFeature, on=\"CustomerID\").rename(columns={\"Qta\":\"Imax\"})\n",
    "    \n",
    "    #The Shannon entropy on the purchasing behaviour of the customer: types of items bought\n",
    "    #Since we are assuming that the type C BasketID entries are a generic cost associated to a CustomerID,\n",
    "    # we can ignore them for the purpouse of computing the entropy since they might refer to a ProdID that cannot\n",
    "    # be bought Ie AMAZONFEE\n",
    "    \n",
    "    probSeriesGrouped = pos_df.groupby([\"CustomerID\"])\\\n",
    "            .apply(lambda x: x.groupby([\"ProdID\"])[\"Qta\"].sum()/x[\"Qta\"].sum())\n",
    "    logSeriesGrouped = np.log2(probSeriesGrouped)\n",
    "    ProdProbLogSeriesGrouped = -1 * probSeriesGrouped * logSeriesGrouped\n",
    "    EFeature = ProdProbLogSeriesGrouped.groupby([\"CustomerID\"]).sum()\n",
    "    EFeature = EFeature.fillna(0)\n",
    "    EFeature.name = \"Eproduct\"\n",
    "    dest_df = dest_df.join(EFeature, on=\"CustomerID\")\n",
    "\n",
    "#Additional features\n",
    "\n",
    "    #Average total revenue per month\n",
    "    #Revenue doesn't take into account the costs (negative Qta)\n",
    "    tot_revenue_cust = pos_df.groupby([\"CustomerID\"]).apply(lambda x: (x[\"Sale\"]*x[\"Qta\"]).sum())\n",
    "    n_month_cust = pos_df.groupby([\"CustomerID\"]).apply(lambda x: x.groupby([\"DateYear\", \"DateMonth\"]).ngroups)\n",
    "    AvgFeature = tot_revenue_cust/n_month_cust\n",
    "    AvgFeature.name = \"AvgRevenueMonth\"\n",
    "    dest_df = dest_df.join(AvgFeature, on=\"CustomerID\")\n",
    "    \n",
    "    #Average total costs per month\n",
    "    #Note: since the entries with costs are a small amount compared to all the others,\n",
    "    # some customers will have NaN as value. We will replace it with 0.\n",
    "    tot_costs_cust = neg_df.groupby([\"CustomerID\"]).apply(lambda x: (x[\"Sale\"]*x[\"Qta\"]).sum())\n",
    "    n_month_cust = neg_df.groupby([\"CustomerID\"]).apply(lambda x: x.groupby([\"DateYear\", \"DateMonth\"]).ngroups)\n",
    "    AvgFeature = (tot_costs_cust/n_month_cust).abs()\n",
    "    AvgFeature.name = \"AvgCostsMonth\"\n",
    "    dest_df = dest_df.join(AvgFeature, on=\"CustomerID\")\n",
    "    dest_df[\"AvgCostsMonth\"] = dest_df[\"AvgCostsMonth\"].fillna(0)\n",
    "\n",
    "    #Average customer shopping sessions per month\n",
    "    #We are interested in finding out about the habits of a customer and given the percentage\n",
    "    # of type C entries compared to standard ones it seems to us that costs are an unusual event\n",
    "    # rather than the norm. Therefore we exclude the type C entries from our definition of shopping session.\n",
    "    tot_sessions_cust = pos_df.groupby([\"CustomerID\"]).apply(lambda x: x[\"BasketID\"].nunique())\n",
    "    n_month_cust = pos_df.groupby([\"CustomerID\"]).apply(lambda x: x.groupby([\"DateYear\", \"DateMonth\"]).ngroups)\n",
    "    AvgFeature = tot_sessions_cust/n_month_cust\n",
    "    AvgFeature.name = \"AvgSessionsMonth\"\n",
    "    dest_df = dest_df.join(AvgFeature, on=\"CustomerID\")\n",
    "\n",
    "    #The Shannon entropy on the purchasing behaviour of the customer: days of the week of shopping\n",
    "    #Justification for type C entries same as for other entropy related feature.\n",
    "    probSeriesGrouped = pos_df.groupby([\"CustomerID\"])\\\n",
    "        .apply(lambda x: x.groupby([\"DateWeekDay\"])[\"BasketID\"].nunique()/x[\"BasketID\"].nunique())\n",
    "    logSeriesGrouped = np.log2(probSeriesGrouped)\n",
    "    ProdProbLogSeriesGrouped = -1 * probSeriesGrouped * logSeriesGrouped\n",
    "    EFeature = ProdProbLogSeriesGrouped.groupby([\"CustomerID\"]).sum()\n",
    "    EFeature.name = \"Eweekday\"\n",
    "    dest_df = dest_df.join(EFeature, on=\"CustomerID\")\n",
    "    \n",
    "    return dest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the new features to both the dataset with and without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_cust_id = df_w_out[\"CustomerID\"].sort_values().unique()[0:-1] #Remove NaN value, last value\n",
    "cust_df_w_out = pd.DataFrame(data=unq_cust_id, columns=[\"CustomerID\"]) #Dataframe containing customer features with outliers\n",
    "\n",
    "unq_cust_id = df_wo_out[\"CustomerID\"].sort_values().unique()[0:-1] #Remove NaN value, last value\n",
    "cust_df_wo_out = pd.DataFrame(data=unq_cust_id, columns=[\"CustomerID\"]) #Dataframe containing customer features without outliers\n",
    "\n",
    "cust_df_w_out[\"CustomerID\"] = cust_df_w_out[\"CustomerID\"].astype(\"object\")\n",
    "cust_df_wo_out[\"CustomerID\"] = cust_df_wo_out[\"CustomerID\"].astype(\"object\")\n",
    "\n",
    "cust_df_w_out = add_features(cust_df_w_out, df_w_out)\n",
    "cust_df_wo_out = add_features(cust_df_wo_out, df_wo_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df_w_out.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df_wo_out.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display correlations that have an absolute value greater than threshold\n",
    "corr_w_out = cust_df_w_out.corr()\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "for col in corr_w_out.columns:\n",
    "    corr_w_out[col] = corr_w_out[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "\n",
    "corr_w_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the outliers, we have a strong correlation between the maximum number of items bought in a single order by a customer and the average revenue per month.  \n",
    "Other potentially interesting correlations are the ones between the average revenue and average costs, and between Imax and average costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display correlations that have an absolute value greater than threshold\n",
    "corr_wo_out = cust_df_wo_out.corr()\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "for col in corr_wo_out.columns:\n",
    "    corr_wo_out[col] = corr_wo_out[col].transform(lambda x: x if abs(x)>threshold else 0)\n",
    "\n",
    "corr_wo_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there is a significant correlation between the max number of items bought and the average expenditure per month of the customer.  \n",
    "This could suggest to us that the customers with the biggest expenditures tend to buy more items per session (further analysis required). (TODO)  \n",
    "The other significant correlations don't seem particularly interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "diff_corr = (cust_df_w_out.corr() - cust_df_wo_out.corr()).abs()\n",
    "\n",
    "for col in diff_corr.columns:\n",
    "    diff_corr[col] = diff_corr[col].transform(lambda x: x if x>threshold else 0)\n",
    "\n",
    "print(\"Difference between the correlation matrices (with and without outliers)\")\n",
    "diff_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the Outlier analysis section, by using the dataset without outliers we lose almost all of the information concerning entries with negative Qta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer-related outliers analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove customer related outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = cust_df.quantile(0.25)\n",
    "Q3 = cust_df.quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fig_height = cust_df.columns.size\n",
    "#fig = plt.figure(figsize=(20, 30)) \n",
    "#fig_dims = (fig_height, 2)\n",
    "#fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "#index = 0\n",
    "\n",
    "for feature in cust_df.columns:\n",
    "    #plt.subplot2grid(fig_dims, (index, 0))\n",
    "    #cust_df.boxplot(feature)\n",
    "\n",
    "    cust_df = cust_df.loc[(cust_df[feature] >= Q1[feature] - 1.5*IQR[feature]) & (cust_df[feature] <= Q3[feature] + 1.5*IQR[feature])]\n",
    "    \n",
    "    #plt.subplot2grid(fig_dims, (index, 1))\n",
    "    #cust_df.boxplot(feature)\n",
    "    #index = index + 1\n",
    "    \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(cust_df, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between Iu and Eweekday is even stronger after removing the outliers.  \n",
    "In order to reduce the dimensionality of the data we will drop Eweekday given the strong similarity to Iu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df = cust_df.drop(\"Eweekday\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task end\n",
    "Save the customer features for the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df_w_out.to_csv(\"customer_features.csv\", sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
